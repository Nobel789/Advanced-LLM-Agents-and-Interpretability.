{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95LsSX78ZTW8"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXVXV2KJc_6z"
      },
      "source": [
        "his notebook is like a special magnifying glass for looking inside a computer program that's really good at writing and understanding words.\n",
        "\n",
        "Imagine the computer program is like a robot that can read and write stories. This notebook helps us see how the robot thinks and makes decisions when it's writing.\n",
        "\n",
        "Here are some of the cool things the notebook lets us do:\n",
        "\n",
        "    Peek inside the robot's brain: We can see all the secret numbers and steps the robot uses when it's trying to understand a sentence or write the next word.\n",
        "    Change the robot's thoughts: We can temporarily change some of the robot's secret numbers to see how that affects what it writes. It's like giving the robot a little nudge to see what happens!\n",
        "    Find special thinking parts: We can look for parts of the robot's brain that are really good at specific jobs, like figuring out who gave something to someone in a sentence.\n",
        "    See how the robot learns: We can look at different versions of the robot from when it was still learning to see how it got so smart at writing.\n",
        "\n",
        "So, the notebook is all about understanding how these amazing word-writing robots work on the inside!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMvPeX3ZTW_"
      },
      "source": [
        "# Transformer Lens Main Demo Notebook\n",
        "\n",
        "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>\n",
        "\n",
        "This is a reference notebook covering the main features of the [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) library for mechanistic interpretability. See [Callum McDougall's tutorial](https://transformerlens-intro.streamlit.app/TransformerLens_&_induction_circuits) for a more structured and gentler introduction to the library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPy1bLCFZTXA"
      },
      "source": [
        "**Tips for reading this Colab:**\n",
        "* You can run all this code for yourself!\n",
        "* The graphs are interactive!\n",
        "* Use the table of contents pane in the sidebar to navigate\n",
        "* Collapse irrelevant sections with the dropdown arrows\n",
        "* Search the page using the search in the sidebar, not CTRL+F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jydS2bQkZTXD"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__DxLTRZTXE",
        "outputId": "4334ea93-fb97-42e2-bc79-db4ab8a2e384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.12/dist-packages (2.16.1)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.10.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.56.1)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.21.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.35.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.9)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.38.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens) (1.1.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.12/dist-packages (1.43.3)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (8.7.0)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (2.8.0+cu126)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.1->circuitsvis) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.1->circuitsvis) (3.0.2)\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "  \u001b[1m\u001b[33m                            DEPRECATION WARNING                            \u001b[m\n",
            "\n",
            "    \u001b[1m\u001b[4m Node.js 16.x is no longer actively supported!\u001b[m\n",
            "\n",
            "  \u001b[1mYou will not receive security or critical stability updates\u001b[m for this version.\n",
            "\n",
            "  You should migrate to a supported version of Node.js as soon as possible.\n",
            "  Use the installation script that corresponds to the version of Node.js you\n",
            "  wish to install. e.g.\n",
            "  \n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_16.x ‚Äî Node.js 16 \"Gallium\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_18.x ‚Äî Node.js 18 \"Hydrogen\" (Maintenance)\u001b[m\n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_19.x ‚Äî Node.js 19 \"Nineteen\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[1m\u001b[32mhttps://deb.nodesource.com/setup_20.x ‚Äî Node.js 20 LTS \"Iron\" (recommended)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_21.x ‚Äî Node.js 21 \"Iron\" (current)\u001b[m\n",
            "   \n",
            "\n",
            "\n",
            "  Please see \u001b[1mhttps://github.com/nodejs/Release\u001b[m for details about which\n",
            "  version may be appropriate for you.\n",
            "\n",
            "  The \u001b[32m\u001b[1mNodeSource\u001b[m Node.js distributions repository contains\n",
            "  information both about supported versions of Node.js and supported Linux\n",
            "  distributions. To learn more about usage, see the repository:\n",
            "   \u001b[4m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "Continuing in 10 seconds ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "DEVELOPMENT_MODE = False\n",
        "# Detect if we're running in Google Colab,This tries to import a Colab-specific module. If it works, you're in Google Colab.\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install if in Colab\n",
        "if IN_COLAB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    # Install a faster Node version\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
        "\n",
        "# Hot reload in development mode & not running on the CD\n",
        "if not IN_COLAB:\n",
        "    from IPython import get_ipython\n",
        "    ip = get_ipython()\n",
        "    if not ip.extension_manager.loaded:\n",
        "        ip.extension_manager.load('autoreload')\n",
        "        %autoreload 2\n",
        "\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12tru0VZpEGu"
      },
      "source": [
        "| üîç **Part**         | üí¨ **What It Does**                                |\n",
        "| ------------------- | -------------------------------------------------- |\n",
        "| `IN_COLAB`          | Detect if you're in Google Colab                   |\n",
        "| `pip install` lines | Install needed packages in Colab                   |\n",
        "| `autoreload`        | Auto-refresh code changes in notebooks (non-Colab) |\n",
        "| `IN_GITHUB`         | Detect if you're running in GitHub Actions         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wgiIszLZTXI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8QJFsdCZTXL"
      },
      "outputs": [],
      "source": [
        "import circuitsvis as cv\n",
        "# Testing that the library works\n",
        "cv.examples.hello(\"Neel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3vA18zjqAne"
      },
      "source": [
        "‚úÖ Explanation:\n",
        "circuitsvis is a visualization library often used with transformer models (like GPT).\n",
        "\n",
        "cv.examples.hello(\"Neel\") runs a demo that outputs something like:\n",
        "\n",
        "üëã Hello, Neel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyEiItekZTXM"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.auto as tqdm\n",
        "import plotly.express as px\n",
        "\n",
        "from jaxtyping import Float\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1GND__iqSvu"
      },
      "source": [
        "| Import              | Purpose                            |\n",
        "| ------------------- | ---------------------------------- |\n",
        "| `torch`, `nn`       | Build and train neural networks    |\n",
        "| `einops`            | Reshape tensors simply             |\n",
        "| `fancy_einsum`      |readable version of Einstein for summation  math         |\n",
        "| `tqdm`              | Show progress bars                 |\n",
        "| `plotly.express`    | Make interactive charts            |\n",
        "| `jaxtyping.Float`   | Type-checking for tensors          |\n",
        "| `functools.partial` | Make reusable function \"templates\" |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Eokx85ZTXO"
      },
      "outputs": [],
      "source": [
        "# import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, FactoredMatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6JbA-DDu05T"
      },
      "source": [
        "| Component           | Purpose                                                      |\n",
        "| ------------------- | ------------------------------------------------------------ |\n",
        "| `utils`             | Helper functions like logit for data/token processing                   |\n",
        "| `HookPoint`         | Insert/remove hooks inside the model       and modify activation functiion to see what impact                   |\n",
        "| `HookedTransformer` | The Transformer model with built-in hook support             |\n",
        "| `FactoredMatrix`    | Analyze matrices (like weights) in a factored/efficient form |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-vrvtKtZTXQ"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhpMHzslZTXS"
      },
      "outputs": [],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8q1TK25vxXa"
      },
      "source": [
        "| Code                            | What It Does                                 |\n",
        "| ------------------------------- | -------------------------------------------- |\n",
        "| `torch.set_grad_enabled(False)` | Globally disables gradients                  |\n",
        "| `with torch.no_grad():`         | Disables gradients **only inside the block** |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWgyLf7wZTXV"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PjlVq7WZTXX"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osF94HvuzjPc"
      },
      "source": [
        "| Function  | Use Case                             | Visualization Type |\n",
        "| --------- | ------------------------------------ | ------------------ |\n",
        "| `imshow`  | Matrices (e.g. attention, weights)   | Heatmap            |\n",
        "| `line`    | Trends (e.g. loss func line, logit line, and activation  values over time) | Line plot          |\n",
        "| `scatter` | Points (e.g. activations, clusters)  | Scatter plot       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QkoiLr8ZTXZ"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgvdhVpZTXa"
      },
      "source": [
        "This is a demo notebook for [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens), **a library I ([Neel Nanda](https://neelnanda.io)) wrote for doing [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models.** The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **If you want to skill up, check out [my guide to getting started](https://neelnanda.io/getting-started), and if you want to jump into an open problem check out my sequence [200 Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems).**\n",
        "\n",
        "I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!\n",
        "\n",
        "The core design principle I've followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1VdnE4v3n57"
      },
      "source": [
        "Simplified text\n",
        "Sure! Here's a simplified version of the main points from the text ‚Äî **explained like you're a curious child**:\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What Is This About?\n",
        "\n",
        "This is a notebook (a kind of special computer document) made by a person named **Neel Nanda**. He built a tool called **TransformerLens** to help understand how smart computer programs (like ChatGPT or GPT-3) actually **think**.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§î The Big Problem\n",
        "\n",
        "Smart models like GPT-3 can talk like humans...\n",
        "But no one really knows **how** they do it.\n",
        "It‚Äôs like having a robot that can do magic tricks, but we don‚Äôt know the secret behind them.\n",
        "\n",
        "Neel finds this frustrating and wants to **figure out the secrets inside** these models.\n",
        "That‚Äôs called **mechanistic interpretability** ‚Äî trying to \"look inside the robot‚Äôs brain\" and understand how it works.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Why He Built TransformerLens\n",
        "\n",
        "Neel was doing research on his own and realized:\n",
        "\n",
        "> ‚ÄúThere are lots of tools to **use** or **train** AI models... but not many to **understand** them.‚Äù\n",
        "\n",
        "So he made **TransformerLens** to:\n",
        "\n",
        "* Let researchers easily explore what's happening inside language models\n",
        "* Help people learn and join this field\n",
        "* Make experiments faster and more fun\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö° Cool Design Idea\n",
        "\n",
        "The best part? This tool helps you test ideas **quickly** ‚Äî like doing a science experiment where you see the results **right away**.\n",
        "That makes research feel like **playtime**!\n",
        "\n",
        "---\n",
        "\n",
        "### üëã Final Message\n",
        "\n",
        "Neel is saying:\n",
        "\n",
        "> \"If you're curious and want to help understand how these smart AIs work ‚Äî **join us!** This is a brand-new field with tons of exciting puzzles to solve.\"\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you'd like this as a cartoon or visual explainer too!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qmO64vbZTXb"
      },
      "source": [
        "## Loading and Running Models\n",
        "\n",
        "TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrnKxVxZTXc"
      },
      "outputs": [],
      "source": [
        "device = utils.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LoYp1KuZTXc"
      },
      "outputs": [],
      "source": [
        "# NBVAL_IGNORE_OUTPUT\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDT5ZoPsh2SZ"
      },
      "source": [
        "\"Hey toolbox, give me the mini version of the talking robot brain, and set it up to run on my computer's fast engine.\"\n",
        "\n",
        "HookedTransformer: This is a special version transforemer(builtin hook ) made for exploring how  GPT model  thinks. Track attention, activations, and other hidden parts\n",
        "\n",
        "‚ÄúHook into‚Äù layers so you can analyze or change what the model does mid-thought\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAtsM_qdZTXe"
      },
      "source": [
        "To try the model out, let's find the loss on this text! Models can be run on a single string or a tensor of tokens (shape: [batch, position], all integers), and the possible return types are:\n",
        "* \"logits\" (shape [batch, position, d_vocab], floats),\n",
        "* \"loss\" (the cross-entropy loss when predicting the next token),\n",
        "* \"both\" (a tuple of (logits, loss))\n",
        "* None (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYu4owRNi49i"
      },
      "source": [
        "Simple understand\n",
        "üß™ What are we trying to do?\n",
        "We want to test the model by giving it some text and seeing how well it understands or predicts it.\n",
        "We do this by checking its loss ‚Äî a number that tells us how wrong the model is when trying to guess the next word or token.\n",
        "\n",
        "üß† How do we give the model input?\n",
        "You can give the model:\n",
        "\n",
        "A string (like \"Hello, how are you?\")\n",
        "\n",
        "Or a tensor of tokens (which is just a fancy list of numbers that represent words or pieces of words)\n",
        "\n",
        "Example shape for tokens:\n",
        "[batch, position]\n",
        "Means: multiple sentences (batch), and the number of words or tokens in each (position)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a17HEVnykQIw"
      },
      "source": [
        "| What You Ask For | What It Means                                                                  |\n",
        "| ---------------- | ------------------------------------------------------------------------------ |\n",
        "| `\"logits\"`       | The model's raw guesses for the next word/token at each step                   |\n",
        "| `\"loss\"`         | A number showing **how wrong** the model was in predicting the next word       |\n",
        "| `\"both\"`         | A pair: the raw guesses **and** the loss number                                |\n",
        "| `None`           | Don‚Äôt return guesses ‚Äî just run the model so we can look at its inner thinking |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEFFwpxdZTXe"
      },
      "outputs": [],
      "source": [
        "model_description_text = \"\"\"## Loading Models\n",
        "\n",
        "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
        "\n",
        "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
        "loss = model(model_description_text, return_type=\"loss\")\n",
        "print(\"Model loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oij29VaQk1RJ"
      },
      "source": [
        "You're giving the model a paragraph and asking:\n",
        "\n",
        "‚ÄúHey smart robot brain, how well can you guess the next word in this text?‚Äù\n",
        "\n",
        "And the model replies with a number called \"loss\" that tells us how well (or badly) it did.\n",
        "\n",
        "üß∏ Analogy:\n",
        "It‚Äôs like giving a kid a sentence with some blanks and seeing how many they get right.\n",
        "\n",
        "\"The cat sat on the ___.\"\n",
        "If they say ‚Äúmat‚Äù, great! Low loss.\n",
        "If they say ‚Äúrefrigerator‚Äù, hmm... high loss!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNgx14AtlEdr"
      },
      "source": [
        " What does a lower loss mean?\n",
        "‚úÖ Small number (like 1.2): The model was pretty good at predicting.\n",
        "\n",
        "‚ùå Big number (like 5.0): The model was way off and didn‚Äôt guess well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEH3Z1lZTXf"
      },
      "source": [
        "## Caching all Activations\n",
        "\n",
        "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out on the first line of the abstract of the GPT-2 paper.\n",
        "\n",
        "<details><summary>On `remove_batch_dim`</summary>\n",
        "\n",
        "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. `gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
        "</details?>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBBHMLXjmj7p"
      },
      "source": [
        "Simple explanation:\n",
        "When we ask the model to think about some words, it does a lot of small calculations inside ‚Äî these are called activations.\n",
        "Caching activations means saving all these little steps so we can look at them later, like recording what‚Äôs happening inside the model‚Äôs brain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bi7GEAXZTXf"
      },
      "outputs": [],
      "source": [
        "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
        "gpt2_tokens = model.to_tokens(gpt2_text)\n",
        "print(gpt2_tokens.device)\n",
        "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODge8zJEncpA"
      },
      "source": [
        "Interpretation the code:\n",
        "\n",
        "You give the model a sentence:\n",
        "\"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets.\"\n",
        "\n",
        "You turn the sentence into tokens with:\n",
        "\n",
        "gpt2_tokens = model.to_tokens(gpt2_text)\n",
        "This means the model changes the words into numbers it can understand.\n",
        "\n",
        "You check where those tokens are stored (like on your computer‚Äôs CPU or GPU):\n",
        "\n",
        "\n",
        "print(gpt2_tokens.device)\n",
        "It tells you if the tokens are on the right place for fast calculations.\n",
        "\n",
        "You run the model with caching to get the guesses (logits) and save all the internal ‚Äúthoughts‚Äù (activations) in gpt2_cache:\n",
        "\n",
        "\n",
        "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
        "You also remove the extra batch layer for easier analysis (remove_batch_dim=True)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWd1mDyWZTXg"
      },
      "source": [
        "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on [Anthropic's PySvelte library](https://github.com/anthropics/PySvelte)).\n",
        "\n",
        "We look this the attention pattern in `gpt2_cache`, an `ActivationCache` object, by entering in the name of the activation, followed by the layer index (here, the activation is called \"attn\" and the layer index is 0). This has shape [head_index, destination_position, source_position], and we use the `model.to_str_tokens` method to convert the text to a list of tokens as strings, since there is an attention weight between each pair of tokens.\n",
        "\n",
        "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n",
        "\n",
        "See the ActivationCache section for more on what `gpt2_cache` can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JsgAbvkZTXh"
      },
      "outputs": [],
      "source": [
        "print(type(gpt2_cache))\n",
        "attention_pattern = gpt2_cache[\"pattern\", 0, \"attn\"]\n",
        "print(attention_pattern.shape)\n",
        "gpt2_str_tokens = model.to_str_tokens(gpt2_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-zplHtJpBo4"
      },
      "source": [
        "üëÄ What are we doing?\n",
        "We‚Äôre going to look inside the model‚Äôs ‚Äúattention‚Äù ‚Äî how the model decides which words to focus on when reading a sentence.\n",
        "\n",
        "üß† What is attention?\n",
        "Imagine when you read a story, sometimes you pay more attention to certain words because they help you understand the meaning better. The model does the same thing! It looks at some words more closely than others.\n",
        "\n",
        "üñºÔ∏è How do we see this?\n",
        "The model has layers, like floors in a building.\n",
        "\n",
        "Each layer has many heads, like little eyes looking at different parts of the sentence.\n",
        "\n",
        "We‚Äôll look at layer 0 (the first floor), at all the heads (all the eyes).\n",
        "\n",
        "üîç What is gpt2_cache?\n",
        "It‚Äôs like a big notebook where the model wrote down all the attention information.\n",
        "\n",
        "We find the attention data by asking for \"attn\" at layer 0.\n",
        "\n",
        "üìè What does the attention pattern look like?\n",
        "It‚Äôs a grid showing how much attention each word gives to every other word.\n",
        "\n",
        "It‚Äôs a lower triangular grid because the model can only pay attention to words that came before in the sentence ‚Äî it can‚Äôt peek into the future!\n",
        "\n",
        "üéÆ Cool part ‚Äî it‚Äôs interactive!\n",
        "You can move your mouse over tokens or heads to see what the model is focusing on, and click to freeze your view.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjTrVReqUUj"
      },
      "source": [
        "1. print(type(gpt2_cache))\n",
        "This line asks: What kind of thing is gpt2_cache?\n",
        "\n",
        "It will print the type (or class) of the object called gpt2_cache.\n",
        "\n",
        "This helps you understand what data or information you‚Äôre working with.\n",
        "\n",
        "2. attention_pattern = gpt2_cache[\"pattern\", 0, \"attn\"]\n",
        "This line gets the attention pattern from the cache.\n",
        "\n",
        "\"pattern\" means we want the attention weights (how much one word looks at another).\n",
        "\n",
        "0 means we want the data from layer 0 (the first layer in the model).\n",
        "\n",
        "\"attn\" tells it we want the attention activations.\n",
        "\n",
        "So, attention_pattern now holds a grid showing which words pay attention to which other words in the first layer.\n",
        "\n",
        "3. print(attention_pattern.shape)\n",
        "This prints the shape (or size) of the attention_pattern data.\n",
        "\n",
        "The shape usually looks like [number_of_heads, destination_positions, source_positions].\n",
        "\n",
        "This tells you how many attention heads there are, and how many tokens (words) the model is looking at.\n",
        "\n",
        "4. gpt2_str_tokens = model.to_str_tokens(gpt2_text)\n",
        "This line turns the original text (gpt2_text) into a list of tokens as strings.\n",
        "\n",
        "Tokens are like little pieces of words or whole words that the model understands.\n",
        "\n",
        "This helps when you want to match the attention pattern to actual words you can read.\n",
        "\n",
        "Summary:\n",
        "You check what gpt2_cache is.\n",
        "\n",
        "You get the attention pattern from layer 0.\n",
        "\n",
        "You check the size of that attention pattern.\n",
        "\n",
        "You get the list of tokens (words) for the original sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we5kZqS5yT9S"
      },
      "source": [
        "How the attention happen\n",
        "üîç 1. What's Being Visualized?\n",
        "\n",
        "This shows attention patterns in the first Transformer layer (Layer 0) of GPT-2. Each pattern shows where a specific attention head is focusing‚Äîthat is, which previous tokens each token \"looks at\" to make its prediction.\n",
        "\n",
        "    Think of it like:\n",
        "    Each token is asking, \"Who should I listen to?\"\n",
        "    Each head gives a different answer!\n",
        "\n",
        "üß† 2. What's an Attention Head?\n",
        "\n",
        "A Transformer layer usually has multiple heads (12 here). Each head learns to focus on different relationships in the sentence.\n",
        "Head #\tMight be focusing on...\n",
        "Head 0\tNearby words (local attention)\n",
        "Head 3\tSyntactic roles (subject, verb)\n",
        "Head 5\tPrevious sentence start\n",
        "Head 8\tPunctuation or sentence boundaries\n",
        "Head 11\tLong-range dependencies\n",
        "üñºÔ∏è 3. Interpreting the Attention Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrApAKz8ZTXi"
      },
      "outputs": [],
      "source": [
        "print(\"Layer 0 Head Attention Patterns:\")\n",
        "cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D7pJShixJRh"
      },
      "source": [
        "3. Interpreting the Attention Maps\n",
        "\n",
        "Each small square is a heatmap (matrix), where:\n",
        "\n",
        "    Rows = destination tokens (which word is paying attention)\n",
        "\n",
        "    Columns = source tokens (which word it's attending to)\n",
        "\n",
        "    Color = strength of attention (brighter = more attention)\n",
        "\n",
        "    üîÅ The diagonal line often means \"pay attention to the previous word\" (common early on in training).\n",
        "\n",
        "    üß¨ Off-diagonal patterns may show dependency structures, like one token attending to its subject or object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNyhR3i9wOnu"
      },
      "source": [
        "üß† Think of a Transformer Layer Like a Team\n",
        "Imagine:\n",
        "\n",
        "A Transformer layer is like a big smart team, and each attention head is a specialist in the team.\n",
        "\n",
        "    Each attention head looks at the input tokens and says:\n",
        "\n",
        "        ‚ÄúBased on what I specialize in, I think token 5 should pay attention to token 2.‚Äù\n",
        "\n",
        "    Each head does this in parallel.\n",
        "\n",
        "    Then their outputs are combined and passed into a feedforward neural network (this part does have neurons).\n",
        "\n",
        "üîç Inside a Transformer Layer\n",
        "\n",
        "Each layer of a Transformer (like GPT-2) has two main parts:\n",
        "Part\tWhat's inside?\tPurpose\n",
        "1. Multi-head Attention\t12 attention heads (each with matrices Q, K, V)\tLets the model ‚Äúlook around‚Äù at other tokens\n",
        "2. Feedforward Network\tThousands of regular neurons (MLP)\tLearns features and transformations\n",
        "\n",
        "So if you see 12 attention heads, that just means:\n",
        "\n",
        "    There are 12 separate attention pathways, each learning different patterns.\n",
        "\n",
        "    Each head has its own parameters (weights), but they are not neurons.\n",
        "\n",
        "üìä Example: GPT-2 Small\n",
        "\n",
        "In GPT-2 small (117M parameters):\n",
        "\n",
        "    Each layer has 12 heads\n",
        "\n",
        "    Each layer also has a feedforward MLP with 3072 neurons\n",
        "\n",
        "    The model has 12 layers total\n",
        "\n",
        "So attention heads are only one part of what‚Äôs going on. Most of the parameters and compute are in the feedforward MLP, not the attention heads!\n",
        "üß† Analogy: Classroom with Advisors\n",
        "\n",
        "    Imagine you're in a classroom.\n",
        "\n",
        "    You ask for help on a question.\n",
        "\n",
        "    You have 12 advisors (attention heads), each specializing in something different (grammar, logic, memory).\n",
        "\n",
        "    Each advisor gives you a suggestion.\n",
        "\n",
        "    You average their help and then pass that into your brain (MLP) to decide the final answer.\n",
        "\n",
        "‚úÖ Summary\n",
        "Concept\tDescription\n",
        "Attention Head\tOne of many ‚Äúfocusing‚Äù mechanisms in each layer that decides which other tokens are important\n",
        "Neuron\tA unit in the MLP part that applies weights, biases, and non-linear activation\n",
        "Layer 0\tThe first Transformer layer, which contains both attention heads and MLP neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2w5uY8Pwnaf"
      },
      "source": [
        "üß™ Curious Next Steps for You:\n",
        "\n",
        "    Try changing the input text and see how attention patterns change.\n",
        "\n",
        "    Freeze on one token (like \"translation\") and explore how all heads treat it.\n",
        "\n",
        "    Look at later layers (e.g., Layer 10) and compare how attention becomes more semantic and structured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLbaKj6dZTXj"
      },
      "source": [
        "In this case, we only wanted the layer 0 attention patterns, but we are storing the internal activations from all layer  locations in the model. It's convenient to have access to all activations, but this can be prohibitively expensive for memory use with larger models, batch sizes, or sequence lengths. In addition, we don't need to do the full forward pass through the model to collect layer 0 attention patterns. The following cell will collect only the layer 0 attention patterns and stop the forward pass at layer 1, requiring far less memory and compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOLX6RNvZTXk"
      },
      "outputs": [],
      "source": [
        "attn_hook_name = \"blocks.0.attn.hook_pattern\"\n",
        "attn_layer = 0\n",
        "_, gpt2_attn_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True, stop_at_layer=attn_layer + 1, names_filter=[attn_hook_name])\n",
        "gpt2_attn = gpt2_attn_cache[attn_hook_name]\n",
        "assert torch.equal(gpt2_attn, attention_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUC6km113kMX"
      },
      "source": [
        "![Activation collection.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XT_Uv8h3oMa"
      },
      "source": [
        "üß† Summary:\n",
        "Goal\tHow It's Done\n",
        "Collect Layer 0 attention patterns only\tUse stop_at_layer=1 and names_filter=[\"blocks.0.attn.hook_pattern\"]\n",
        "Avoid storing all layers' activations\t‚úÖ Much faster and lower memory\n",
        "Compare to full run\tUse torch.equal() to confirm equivalence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1KOwNli4O7J"
      },
      "source": [
        "üß† Why Do We Use Softmax in the Output Layer of Standard Neural Nets?\n",
        "\n",
        "In a classification neural net (e.g., image classifier), the last layer produces logits (raw scores), and softmax turns them into probabilities across classes.\n",
        "\n",
        "    That‚Äôs where we make the final decision (e.g., cat vs dog).\n",
        "\n",
        "    We train it with cross-entropy loss, which expects probabilities.\n",
        "\n",
        "‚úÖ So softmax at the output makes sense for class predictions.\n",
        "üîÅ But in a Transformer, We Use Softmax in the Attention Layer ‚Äî Why?\n",
        "Key insight:\n",
        "\n",
        "    We don‚Äôt use softmax to make a final prediction,\n",
        "    We use it to decide how much attention one token pays to others.\n",
        "\n",
        "Let me explain visually:\n",
        "üîó Attention Mechanism Recap\n",
        "\n",
        "Each token wants to ask:\n",
        "\n",
        "    \"Whom should I listen to from the other tokens?\"\n",
        "\n",
        "It computes attention scores between itself and every other token (dot products of query and key vectors).\n",
        "These scores could be any real number, even negative.\n",
        "\n",
        "We apply softmax to these scores to turn them into attention weights:\n",
        "Example:\n",
        "Token Pair\tRaw Score\tSoftmax Weight\n",
        "\"The\" attends to \"cat\"\t2.0\t0.73\n",
        "\"The\" attends to \"sat\"\t1.0\t0.27\n",
        "\n",
        "Now the model will:\n",
        "\n",
        "    Take 73% of the info from \"cat\"\n",
        "\n",
        "    Take 27% of the info from \"sat\"\n",
        "\n",
        "This is how attention works.\n",
        "\n",
        "    üß† Softmax makes attention selective and interpretable.\n",
        "\n",
        "‚úÖ Why Softmax in Layer 0 (and all attention layers)?\n",
        "\n",
        "Even in Layer 0, the model must decide where to focus in the input sentence.\n",
        "\n",
        "Without softmax:\n",
        "\n",
        "    Attention scores could be unbounded (bad for training)\n",
        "\n",
        "    No normalization = no probabilistic interpretation\n",
        "\n",
        "    Could lead to unstable or arbitrary token mixing\n",
        "\n",
        "With softmax:\n",
        "\n",
        "    Token attends more strongly to relevant others\n",
        "\n",
        "    Prevents exploding weights or \"fuzzy\" mixing\n",
        "\n",
        "    Enables multiple heads to learn different focus patterns\n",
        "\n",
        "üìå Summary Table\n",
        "Where is Softmax Used?\tWhy Use It?\n",
        "Output layer of classifier\tConvert logits to class probabilities\n",
        "Attention mechanism (all layers, incl. Layer 0)\tTurn raw similarity scores into attention weights ‚Äî to focus!\n",
        "ü§ì Bonus: Could We Use Other Functions Instead of Softmax?\n",
        "\n",
        "Yes! Researchers have explored:\n",
        "\n",
        "    Sparsemax (outputs sparse weights)\n",
        "\n",
        "    Entmax (generalizes softmax to be smoother or sharper)\n",
        "\n",
        "    Linear attention (avoids softmax for speed)\n",
        "\n",
        "But softmax is still the most common because:\n",
        "\n",
        "    It‚Äôs smooth, differentiable\n",
        "\n",
        "    Easy to train\n",
        "\n",
        "    Works well empirically\n",
        "\n",
        "üéì Final Thought\n",
        "\n",
        "    Softmax in attention is not for prediction, it's for deciding where to look.\n",
        "    It turns attention scores into a focused distribution over other tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wns5hE3lZTXk"
      },
      "source": [
        "## Hooks: Intervening on Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBbR22ZhZTXk"
      },
      "source": [
        "One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us eg set up careful counterfactuals and causal intervention to easily understand model behaviour.\n",
        "\n",
        "Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it.\n",
        "\n",
        "We do this by adding a **hook function** to that activation. The hook function maps `current_activation_value, hook_point` to `new_activation_value`. As the model is run, it computes that activation as normal, and then the hook function is applied to compute a replacement, and that is substituted in for the activation. The hook function can be an arbitrary Python function, so long as it returns a tensor of the correct shape.\n",
        "\n",
        "<details><summary>Relationship to PyTorch hooks</summary>\n",
        "\n",
        "[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).\n",
        "\n",
        "They also come with a range of other quality of life improvements, like the model having a `model.reset_hooks()` method to remove all hooks, or helper methods to temporarily add hooks for a single forward pass - it is *incredibly* easy to shoot yourself in the foot with standard PyTorch hooks!\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxCAlDQRNlYx"
      },
      "source": [
        "Neural networks = robot brains we can control.\n",
        "\n",
        "TransformerLens = a microscope for those brains.\n",
        "\n",
        "Hook points = trapdoors we can use to see or change what the brain is thinking.\n",
        "\n",
        "Hook functions = little rules that decide what to change.\n",
        "\n",
        "PyTorch hooks = similar, but messier and less precise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF_EcMAlGhjQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZCyQop7ZTXl"
      },
      "source": [
        "As a basic example, let's [ablate](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx) head 7 in layer 0 on the text above.\n",
        "\n",
        "We define a `head_ablation_hook` function. This takes the value tensor for attention layer 0, and sets the component with `head_index==7` to zero and returns it (Note - we return by convention, but since we're editing the activation in-place, we don't strictly *need* to).\n",
        "\n",
        "We then use the `run_with_hooks` helper function to run the model and *temporarily* add in the hook for just this run. We enter in the hook as a tuple of the activation name (also the hook point name - found with `utils.get_act_name`) and the hook function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkeeXHamPE6e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLlSoVCdPExI"
      },
      "source": [
        "Simple explanation aboutHead_abalation hook function\n",
        "üß† What's Happening Here?\n",
        "We‚Äôre doing something really specific and cool:\n",
        "We‚Äôre going to \"ablate\" (turn off) one tiny thinking part of the robot brain: head 7 in layer 0.\n",
        "\n",
        "Imagine the brain is made up of layers of mini brains called attention heads. Each head looks at the words you give it and tries to figure out what's important.\n",
        "\n",
        "So in this example, we say:\n",
        "\n",
        "\"Let‚Äôs turn off head number 7 in the first layer and see what changes in the robot‚Äôs thinking!\"\n",
        "\n",
        "‚öôÔ∏è How Do We Do That?\n",
        "We write a hook function called head_ablation_hook.\n",
        "\n",
        "This is like writing a rule that says:\n",
        "\n",
        "‚ÄúHey! If you're head number 7, stop thinking. Just be zero ‚Äî like you‚Äôre asleep.‚Äù\n",
        "\n",
        "In code, this means:\n",
        "We look at a big list of what all the heads are thinking (this is called a tensor, like a magic spreadsheet), and we set the row for head 7 to zero.\n",
        "\n",
        "We run the model using run_with_hooks.\n",
        "\n",
        "That‚Äôs like running the robot brain with your hook active ‚Äî but just this one time.\n",
        "\n",
        "We say:\n",
        "\n",
        "‚ÄúHey robot, think about the sentence we gave you. But this time, head 7 in layer 0 isn‚Äôt allowed to help. Let‚Äôs see what you say.‚Äù\n",
        "\n",
        "üõ†Ô∏è Behind the Scenes\n",
        "To tell the model exactly where to put the hook, we give it two things:\n",
        "\n",
        "A hook point name ‚Äî this is just the label that says:\n",
        "\n",
        "‚ÄúGo to layer 0, in the attention part, before outputting anything.‚Äù\n",
        "\n",
        "We find that name using something like utils.get_act_name.\n",
        "\n",
        "The hook function ‚Äî that‚Äôs our custom rule that zeros out head 7.\n",
        "\n",
        "üß™ Why is This Cool?\n",
        "It‚Äôs like doing science on a brain! You can:\n",
        "\n",
        "Turn off one part and see how it behaves differently.\n",
        "\n",
        "Learn which parts matter for certain thoughts.\n",
        "\n",
        "Find out which heads do grammar, which do names, or which love talking about cats üò∫\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kxqNVX8ZTXl"
      },
      "outputs": [],
      "source": [
        "layer_to_ablate = 0\n",
        "head_index_to_ablate = 8\n",
        "\n",
        "# We define a head ablation hook\n",
        "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
        "#\n",
        "def head_ablation_hook(\n",
        "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
        "    hook: HookPoint\n",
        ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
        "    print(f\"Shape of the value tensor: {value.shape}\")\n",
        "    value[:, :, head_index_to_ablate, :] = 0.\n",
        "    return value\n",
        "\n",
        "original_loss = model(gpt2_tokens, return_type=\"loss\")\n",
        "ablated_loss = model.run_with_hooks(\n",
        "    gpt2_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        utils.get_act_name(\"v\", layer_to_ablate),\n",
        "        head_ablation_hook\n",
        "        )]\n",
        "    )\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGUbSnN9Sk6r"
      },
      "source": [
        "ü§ñ What Is This Code Doing?\n",
        "You‚Äôre giving the robot (the model) some words, then:\n",
        "\n",
        "Measuring how well it understands them. (That‚Äôs the original_loss)\n",
        "\n",
        "Turning off one tiny part of its brain. (Head 8 in Layer 0)\n",
        "\n",
        "Measuring again. (That‚Äôs the ablated_loss)\n",
        "\n",
        "Comparing the two to see if turning that part off made the robot worse at understanding.\n",
        "\n",
        "üß† What‚Äôs an Attention Head?\n",
        "Remember: each layer in the robot brain has several little mini-brains called attention heads. Each one looks at the sentence and pays attention to different things ‚Äî maybe grammar, important words, or sentence structure.\n",
        "\n",
        "This code turns off head 8 in layer 0 ‚Äî we‚Äôre basically telling that mini-brain:\n",
        "\n",
        "‚ÄúShh! You‚Äôre not allowed to think this time.‚Äù\n",
        "\n",
        "üéØ Finding the Right Spot\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "utils.get_act_name(\"v\", layer_to_ablate)\n",
        "This finds the exact place in the robot brain where the attention heads produce their output (the \"V\" part in \"QKV\" ‚Äî a key part of attention). You‚Äôre saying:\n",
        "\n",
        "‚ÄúHook into the ‚Äòvalue‚Äô output of layer 0.‚Äù\n",
        "\n",
        "üß™ Running the Experiment\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "original_loss = model(gpt2_tokens, return_type=\"loss\")\n",
        "First, you run the model normally and check the loss ‚Äî which is a score for how wrong it was (lower is better).\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "ablated_loss = model.run_with_hooks(\n",
        "    gpt2_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(hook_point_name, head_ablation_hook)]\n",
        ")\n",
        "Then you run it again, but this time with your hook in place ‚Äî head 8 is now silent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePjomLokUN6b"
      },
      "source": [
        "Important nootes about HOOK function use :\n",
        "**bold text**\n",
        "Hooks change what the model does ‚Äî like spies in the brain.\n",
        "\n",
        "run_with_hooks adds them temporarily.\n",
        "\n",
        "But if there's an error, they might stay stuck.\n",
        "\n",
        "Use model.reset_hooks() to clean up if that happens.\n",
        "\n",
        "Use add_perma_hook() only if you want them to stay on purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzHPIx0ZTXo"
      },
      "source": [
        "### Activation Patching on the Indirect Object Identification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgD4KM8vZTXr"
      },
      "source": [
        "For a somewhat more involved example, let's use hooks to apply **[activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** on the **[Indirect Object Identification](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa)** (IOI) task.\n",
        "\n",
        "The IOI task is the task of identifying that a sentence like \"After John and Mary went to the store, Mary gave a bottle of milk to\" continues with \" John\" rather than \" Mary\" (ie, finding the indirect object), and Redwood Research have [an excellent paper studying the underlying circuit in GPT-2 Small](https://arxiv.org/abs/2211.00593).\n",
        "\n",
        "**[Activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** is a technique from [Kevin Meng and David Bau's excellent ROME paper](https://rome.baulab.info/). The goal is to identify which model activations are important for completing a task. We do this by setting up a **clean prompt** and a **corrupted prompt** and a **metric** for performance on the task. We then pick a specific model activation, run the model on the corrupted prompt, but then *intervene* on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance.\n",
        "(See [a more detailed demonstration of activation patching here](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzckuxiaZTXs"
      },
      "source": [
        "Here, our clean prompt is \"After John and Mary went to the store, **Mary** gave a bottle of milk to\", our corrupted prompt is \"After John and Mary went to the store, **John** gave a bottle of milk to\", and our metric is the difference between the correct logit ( John) and the incorrect logit ( Mary) on the final token.\n",
        "\n",
        "We see that the logit difference is significantly positive on the clean prompt, and significantly negative on the corrupted prompt, showing that the model is capable of doing the task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02KdSXaY3_C"
      },
      "source": [
        "Awesome! You're now stepping into activation patching, one of the coolest and most powerful tools in transformer interpretability ‚Äî like brain surgery with a scalpel and microscope!\n",
        "\n",
        "üß™ What Is the IOI Task?\n",
        "Let‚Äôs start with the task first.\n",
        "\n",
        "You‚Äôre asking the model to fill in the blank:\n",
        "\n",
        "‚ÄúAfter John and Mary went to the store, Mary gave a bottle of milk to ___‚Äù\n",
        "\n",
        "Who should go in the blank?\n",
        "‚úÖ John, because he‚Äôs receiving the milk.\n",
        "‚ùå Not Mary, because she‚Äôs the one giving it.\n",
        "\n",
        "That‚Äôs called Indirect Object Identification (IOI) ‚Äî figuring out who is getting something.\n",
        "\n",
        "üß† What‚Äôs Activation Patching?\n",
        "Imagine running two different versions of the same story through your robot brain:\n",
        "\n",
        "Clean Prompt: the story makes sense\n",
        "\n",
        "\"...Mary gave...to __\" ‚Üí should continue with John\n",
        "\n",
        "Corrupted Prompt: a confusing version\n",
        "\n",
        "\"...John gave...to __\" ‚Üí makes the model more likely to wrongly pick Mary\n",
        "\n",
        "Now here‚Äôs the magic:\n",
        "\n",
        "We let the model read the corrupted prompt, but at one specific moment in the brain‚Äôs process, we say:\n",
        "\n",
        "‚ÄúWait! At this exact spot, copy the thought it had from the clean story instead.‚Äù\n",
        "\n",
        "That‚Äôs activation patching ‚Äî swapping out just one piece of the corrupted brain state with the clean one, and seeing if that makes the model act more correctly.\n",
        "\n",
        "It‚Äôs like saying:\n",
        "\n",
        "‚ÄúIf I just fix this one memory, does the model remember who got the milk?‚Äù\n",
        "\n",
        "üîç What‚Äôs the Metric?\n",
        "We want to measure how well the model is doing the task. So we look at the logits ‚Äî the model‚Äôs raw guesses ‚Äî and compare:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "logit_diff = logit[\"John\"] - logit[\"Mary\"]\n",
        "‚úÖ On the clean prompt, this number should be positive ‚Üí John is more likely.\n",
        "\n",
        "‚ùå On the corrupted prompt, this number is negative ‚Üí Mary is more likely.\n",
        "\n",
        "If the patched version goes back to being positive, then the thing we patched was important!\n",
        "\n",
        "üß™ The Recipe for Activation Patching\n",
        "Let‚Äôs summarize the steps:\n",
        "\n",
        "Set up your prompts:\n",
        "\n",
        "Clean: ‚ÄúMary gave a bottle of milk to ___‚Äù\n",
        "\n",
        "Corrupted: ‚ÄúJohn gave a bottle of milk to ___‚Äù\n",
        "\n",
        "Run both through the model.\n",
        "\n",
        "Save the activation values from the clean run.\n",
        "\n",
        "Use a hook to replace a specific activation in the corrupted run with the clean one.\n",
        "\n",
        "Measure the effect.\n",
        "\n",
        "If the model‚Äôs logit difference moves toward the clean version, then that activation matters!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFnom3RTZTXt"
      },
      "outputs": [],
      "source": [
        "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
        "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
        "\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
        "\n",
        "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
        "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
        "    # If the string is not a single token, it raises an error.\n",
        "    correct_index = model.to_single_token(correct_answer)\n",
        "    incorrect_index = model.to_single_token(incorrect_answer)\n",
        "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
        "\n",
        "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
        "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
        "\n",
        "# We don't need to cache on the corrupted prompt.\n",
        "corrupted_logits = model(corrupted_tokens)\n",
        "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
        "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfsQrkIPcV3j"
      },
      "source": [
        "What code do:\n",
        "Perfect! You're now doing a **real activation patching setup**, and you've just completed the **first half** of the experiment: collecting the clean and corrupted outputs.\n",
        "\n",
        "Let‚Äôs walk through what this code is doing and why it's so cool ‚Äî step by step, in kid-curious style:\n",
        "\n",
        "---\n",
        "\n",
        "## üßº Step 1: Define the Prompts\n",
        "\n",
        "```python\n",
        "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
        "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
        "```\n",
        "\n",
        "You're telling **two slightly different stories** to the model:\n",
        "\n",
        "* ‚úÖ In the clean one, **Mary** gives milk ‚Üí model should guess **John** is the recipient.\n",
        "* ‚ùå In the corrupted one, **John** gives milk ‚Üí model might wrongly guess **Mary**.\n",
        "\n",
        "You're setting up a situation where **only one word changes**, but it **confuses** the model.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step 2: Turn Prompts into Tokens\n",
        "\n",
        "```python\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
        "```\n",
        "\n",
        "The model doesn‚Äôt understand plain words. This step turns the prompts into **token numbers** ‚Äî like turning words into puzzle pieces it can work with.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Step 3: Define the Metric (Logit Difference)\n",
        "\n",
        "```python\n",
        "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
        "    correct_index = model.to_single_token(correct_answer)\n",
        "    incorrect_index = model.to_single_token(incorrect_answer)\n",
        "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
        "```\n",
        "\n",
        "This is the **score system**.\n",
        "\n",
        "* The model makes a guess about **what the next word should be** (called a *logit* ‚Äî bigger means \"more likely\").\n",
        "* You compare:\n",
        "\n",
        "  * How likely is **John**?\n",
        "  * How likely is **Mary**?\n",
        "* A **positive score** means it's doing the task well.\n",
        "* A **negative score** means it's getting confused.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step 4: Run the Clean Prompt & Cache Activations\n",
        "\n",
        "```python\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "```\n",
        "\n",
        "Here‚Äôs the big move!\n",
        "\n",
        "You're running the model on the clean story and saying:\n",
        "\n",
        "> ‚ÄúSave every thought the model had at every layer ‚Äî so I can borrow one later.‚Äù\n",
        "\n",
        "You store that in `clean_cache`.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Step 5: Run the Corrupted Prompt (No Cache Needed)\n",
        "\n",
        "```python\n",
        "corrupted_logits = model(corrupted_tokens)\n",
        "```\n",
        "\n",
        "Now you run the **wrong version**, without saving anything ‚Äî just seeing how badly it messes up.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Step 6: Measure the Differences\n",
        "\n",
        "```python\n",
        "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
        "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n",
        "```\n",
        "\n",
        "You print the score for both versions.\n",
        "\n",
        "* ‚úÖ Clean: Should be **positive** ‚Üí ‚ÄúJohn‚Äù more likely than ‚ÄúMary‚Äù.\n",
        "* ‚ùå Corrupted: Should be **negative** ‚Üí the model wrongly favors ‚ÄúMary‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeppe-xbZTXu"
      },
      "source": [
        "We now setup the hook function to do activation patching. Here, we'll patch in the [residual stream](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=DHp9vZ0h9lA9OCrzG2Y3rrzH) at the start of a specific layer and at a specific position. This will let us see how much the model is using the residual stream at that layer and position to represent the key information for the task.\n",
        "\n",
        "We want to iterate over all layers and positions, so we write the hook to take in an position parameter. Hook functions must have the input signature (activation, hook), but we can use `functools.partial` to set the position parameter before passing it to `run_with_hooks`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yjwhEHwd2aC"
      },
      "source": [
        "**surgical part of activation patching**:\n",
        "Awesome ‚Äî now you‚Äôre moving into the surgical part of activation patching: using a hook to replace part of the model's memory (the residual stream) during a run!\n",
        "\n",
        "Let‚Äôs break this down so it‚Äôs crystal clear, even for someone new but curious.\n",
        "\n",
        "üß† What Is the Residual Stream?\n",
        "Think of the residual stream like the model‚Äôs notebook ‚Äî every time it thinks something, it writes it down. Every layer reads from and adds to this notebook.\n",
        "\n",
        "By patching the residual stream, you‚Äôre saying:\n",
        "\n",
        "‚ÄúAt this layer and this position (word), replace what the corrupted prompt wrote in the notebook with what the clean prompt wrote instead.‚Äù\n",
        "\n",
        "If doing that fixes the model‚Äôs answer, it means that specific note was really important!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsSirM-9ZTXu"
      },
      "outputs": [],
      "source": [
        "# We define a residual stream patching hook\n",
        "# We choose to act on the residual stream at the start of the layer, so we call it resid_pre\n",
        "# The type annotations are a guide to the reader and are not necessary\n",
        "def residual_stream_patching_hook(\n",
        "    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    hook: HookPoint,\n",
        "    position: int\n",
        ") -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "    # Each HookPoint has a name attribute giving the name of the hook.\n",
        "    clean_resid_pre = clean_cache[hook.name]\n",
        "    resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
        "    return resid_pre\n",
        "\n",
        "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "num_positions = len(clean_tokens[0])\n",
        "ioi_patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
        "\n",
        "for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
        "    for position in range(num_positions):\n",
        "        # Use functools.partial to create a temporary hook function with the position fixed\n",
        "        temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
        "        # Run the model with the patching hook\n",
        "        patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
        "            (utils.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
        "        ])\n",
        "        # Calculate the logit difference\n",
        "        patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n",
        "        # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
        "        ioi_patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff)/(clean_logit_diff - corrupted_logit_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46h4D-blTyX_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.imshow(ioi_patching_result.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
        "\n",
        "# Add labels\n",
        "plt.colorbar(label=\"Patch Effect (0 = no effect, 1 = full recovery)\")\n",
        "plt.xlabel(\"Position in Prompt\")\n",
        "plt.ylabel(\"Layer\")\n",
        "plt.title(\"Activation Patching Effect on Residual Stream (IOI Task)\")\n",
        "\n",
        "# Show it\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8iLCRpqdB0F"
      },
      "source": [
        "0   1    2    3     4    5      6     7     8     9     10    11    12    13    14     15    16\n",
        "After John and Mary went to  the  store, Mary gave  a   bottle  of    milk   to\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKYnAKgGcJZz"
      },
      "source": [
        "Simplyfying how Activation path work:\n",
        "üîß What Is Activation Patching?\n",
        "Imagine the AI model is thinking through a puzzle, step by step. At each step (layer), it's writing little notes to itself (these are called activations).\n",
        "\n",
        "Sometimes, it makes a mistake because it got confused earlier in the prompt.\n",
        "\n",
        "üîç Activation patching is like this:\n",
        "\n",
        "You look at how the model behaves when it's right (the clean prompt), and when it's wrong (the corrupted prompt).\n",
        "Then you copy a small piece of the model's thoughts from the right run and paste it into the wrong one ‚Äî and see if that helps fix the mistake.\n",
        "\n",
        "üé® Imagine It Like This:\n",
        "üß† The model is solving this sentence:\n",
        "\n",
        "After John and Mary went to the store, Mary gave a bottle of milk to ___.\n",
        "\n",
        "It has to figure out who the milk is going to, not who is giving it.\n",
        "\n",
        "Now, you break the sentence like this:\n",
        "\n",
        "After John and Mary went to the store, John gave a bottle of milk to ___.\n",
        "\n",
        "Now it might guess \"Mary\" (wrong!) instead of \"John\" (right!).\n",
        "\n",
        "‚ú® What You Do With Patching\n",
        "You:\n",
        "\n",
        "Run the correct version (\"Mary gave...\") ‚Üí save the model's activations.\n",
        "\n",
        "Run the broken version (\"John gave...\") ‚Üí but at a specific point (say, word 10 in layer 5), sneak in the activation from the correct version.\n",
        "\n",
        "See if the model now gets the answer right!\n",
        "\n",
        "If it does ‚Äî bingo! That part of the model was important for making the correct decision.\n",
        "\n",
        "üí° Why It‚Äôs Cool\n",
        "Activation patching helps us figure out:\n",
        "\n",
        "Where in the model‚Äôs brain the decision is being made\n",
        "\n",
        "Which word and which layer helped it get the right answer\n",
        "\n",
        "It's like debugging a robot‚Äôs thought process, one piece at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHN0T7HoZTXv"
      },
      "source": [
        "We can now visualize the results, and see that this computation is extremely localised within the model. Initially, the second subject (Mary) token is all that matters (naturally, as it's the only different token), and all relevant information remains here until heads in layer 7 and 8 move this to the final token where it's used to predict the indirect object.\n",
        "(Note - the heads are in layer 7 and 8, not 8 and 9, because we patched in the residual stream at the *start* of each layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe0bWz-YdKJz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAB1Z3yJZTXw"
      },
      "outputs": [],
      "source": [
        "# Add the index to the end of the label, because plotly doesn't like duplicate labels\n",
        "token_labels = [f\"{token}_{index}\" for index, token in enumerate(model.to_str_tokens(clean_tokens))]\n",
        "imshow(ioi_patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-haC9vqgc1c"
      },
      "source": [
        "Dark blue = Patch helps a lot (value close to 1)\n",
        "\n",
        "White = Patch has no effect (value around 0)\n",
        "\n",
        "Red = Patch hurts performance (value near -1)\n",
        "\n",
        "üß† Interpretation of the Current Image:\n",
        "Strong blue vertical bar at Mary_10 (position 10):\n",
        "\n",
        "The model heavily relies on the residual stream at that token.\n",
        "\n",
        "Particularly in layers 0 through 7, patching this token recovers the correct behavior.\n",
        "\n",
        "Matches the fact that ‚ÄúMary‚Äù is the key difference in the two prompts.\n",
        "\n",
        "Second important area: to_16 (last token):\n",
        "\n",
        "In layers 9‚Äì11, patching to_16 helps, but less strongly than Mary_10.\n",
        "\n",
        "This likely reflects where the model actually makes the prediction, so late layers matter there.\n",
        "\n",
        "Most of the rest of the matrix is white:\n",
        "\n",
        "Means patching had no effect at those locations.\n",
        "\n",
        "The model isn't using those activations for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmp5hqOBZTXw"
      },
      "source": [
        "## Hooks: Accessing Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82NXMLEsZTXx"
      },
      "source": [
        "Hooks can also be used to just **access** an activation - to run some function using that activation value, *without* changing the activation value. This can be achieved by just having the hook return nothing, and not editing the activation in place.\n",
        "\n",
        "This is useful for eg extracting activations for a specific task, or for doing some long-running calculation across many inputs, eg finding the text that most activates a specific neuron. (Note - everything this can do *could* be done with `run_with_cache` and post-processing, but this workflow can be more intuitive and memory efficient.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O3KZ5AeZTXx"
      },
      "source": [
        "To demonstrate this, let's look for **[induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)** in GPT-2 Small.\n",
        "\n",
        "Induction circuits are a very important circuit in generative language models, which are used to detect and continue repeated subsequences. They consist of two heads in separate layers that compose together, a **previous token head** which always attends to the previous token, and an **induction head** which attends to the token *after* an earlier copy of the current token.\n",
        "\n",
        "To see why this is important, let's say that the model is trying to predict the next token in a news article about Michael Jordan. The token \" Michael\", in general, could be followed by many surnames. But an induction head will look from that occurrence of \" Michael\" to the token after previous occurrences of \" Michael\", ie \" Jordan\" and can confidently predict that that will come next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgd7bMl9ZTXy"
      },
      "source": [
        "An interesting fact about induction heads is that they generalise to arbitrary sequences of repeated tokens. We can see this by generating sequences of 50 random tokens, repeated twice, and plotting the average loss at predicting the next token, by position. We see that the model goes from terrible to very good at the halfway point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMquRn6LhmM9"
      },
      "source": [
        "Simple explanation\n",
        "Imagine your brain is doing homework and someone can watch it work‚Äîbut without poking or changing anything. That‚Äôs kind of what these \"hooks\" do with a language model‚Äôs brain. They just peek at what‚Äôs happening inside without messing anything up!\n",
        "\n",
        "Now, here‚Äôs the fun part: Inside the model‚Äôs brain, there are special helpers called induction heads. These helpers are really good at spotting patterns and continuing them. üß†‚ú®\n",
        "\n",
        "Let‚Äôs say you‚Äôre reading something like:\n",
        "\n",
        "\"Michael Jordan is a basketball player. Michael...\"\n",
        "\n",
        "Your brain might think, ‚ÄúHmm, last time ‚ÄòMichael‚Äô was followed by ‚ÄòJordan‚Äô, so maybe that comes next!‚Äù\n",
        "That's exactly what an induction head does!\n",
        "\n",
        "To test how smart the model is at this pattern-finding trick, researchers give it a string of 50 nonsense words (like: ‚Äúflub blip zork...‚Äù) and then repeat the same 50 words. They watch the model's guesses. At first, it's bad at guessing‚Äîbut right at the halfway point, it gets really good! That shows the model noticed, ‚ÄúOh! I‚Äôve seen these before‚ÄîI know what comes next!‚Äù\n",
        "\n",
        "So in short:\n",
        "\n",
        "Hooks help us watch the model‚Äôs thoughts üïµÔ∏è\n",
        "\n",
        "Induction heads are the model‚Äôs pattern detectors üß©\n",
        "\n",
        "They help the model guess smartly when it sees repeats üåÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRfXnO5MbEAK"
      },
      "source": [
        "Comprehensive Summary of code:\n",
        "\n",
        "This script generates a synthetic batch of random token sequences, repeats each sequence to double its length, and feeds these repeated sequences into a model to obtain predictions. The model's loss function is used to compute the log-probability loss for each token, and then the average loss is calculated for each position across the batch. Finally, the script visualizes how the model's loss varies by token position in the repeated sequences. This can be useful for diagnosing how well the model handles repeated or long sequences, and for identifying if there's a positional bias in the model‚Äôs predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FFBEYOqZTXy"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "seq_len = 50\n",
        "size = (batch_size, seq_len)\n",
        "input_tensor = torch.randint(1000, 10000, size)\n",
        "\n",
        "random_tokens = input_tensor.to(model.cfg.device)\n",
        "repeated_tokens = einops.repeat(random_tokens, \"batch seq_len -> batch (2 seq_len)\")\n",
        "repeated_logits = model(repeated_tokens)\n",
        "correct_log_probs = model.loss_fn(repeated_logits, repeated_tokens, per_token=True)\n",
        "loss_by_position = einops.reduce(correct_log_probs, \"batch position -> position\", \"mean\")\n",
        "line(loss_by_position, xaxis=\"Position\", yaxis=\"Loss\", title=\"Loss by position on random repeated tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Wqjn31bBH6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUJUOv6CZTXy"
      },
      "source": [
        "The induction heads will be attending from the second occurrence of each token to the token *after* its first occurrence, ie the token `50-1==49` places back. So by looking at the average attention paid 49 tokens back, we can identify induction heads! Let's define a hook to do this!\n",
        "\n",
        "<details><summary>Technical details</summary>\n",
        "\n",
        "* We attach the hook to the attention pattern activation. There's one big pattern activation per layer, stacked across all heads, so we need to do some tensor manipulation to get a per-head score.\n",
        "* Hook functions can access global state, so we make a big tensor to store the induction head score for each head, and then we just add the score for each head to the appropriate position in the tensor.\n",
        "* To get a single hook function that works for each layer, we use the `hook.layer()` method to get the layer index (internally this is just inferred from the hook names).\n",
        "* As we want to add this to *every* activation pattern hook point, rather than giving the string for an activation name, this time we give a **name filter**. This is a Boolean function on hook point names, and it adds the hook function to every hook point where the function evaluates as true.\n",
        "    * `run_with_hooks` allows us to enter a list of (act_name, hook_function) pairs to all be added at once, so we could also have done this by inputting a list with a hook for each layer.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m0sp9K-dCVk"
      },
      "source": [
        "Copilot said: This plot shows the \"Loss by position\n",
        "\n",
        "This plot shows the \"Loss by position on random repeated tokens\" for a sequence processed by a machine learning model (likely a language model). Here‚Äôs a detailed explanation:\n",
        "Line-by-Line Analysis\n",
        "\n",
        "    X-axis (\"index\"): Represents the position of each token in the sequence. There are 100 positions (0 to 99), which matches the repeated sequence length from your code (original sequence length of 50, repeated to make 100).\n",
        "    Y-axis (\"value\"): Represents the average loss at each position across the batch. Loss is a measure of how well the model predicts the token at each position‚Äîa higher loss means worse prediction.\n",
        "    Title: Indicates this is a plot of loss per position for random, repeated tokens.\n",
        "\n",
        "What the Plot Shows\n",
        "\n",
        "    Positions 0‚Äì49: The loss is consistently high (around 11‚Äì15). In this first half, the model is predicting random tokens it has likely never seen together, so its predictions are essentially guesses, resulting in high loss values.\n",
        "    Position 50 (transition point): There is a dramatic drop in the loss.\n",
        "    Positions 50‚Äì99: The loss drops to near zero and stays low for the remainder of the sequence. This region corresponds to the repeated part of the sequence: the tokens here are an exact copy of the first 50. Since the model has already seen these tokens in the first half, it can predict them almost perfectly, resulting in very low loss.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "    Why is the loss so high in the first half?\n",
        "        The tokens are completely random and novel to the model, so it cannot predict them well.\n",
        "    Why does the loss drop in the second half?\n",
        "        The sequence repeats, so the model has already seen the tokens and can use information from the first half to predict the second half with high accuracy.\n",
        "    What does this tell us about the model?\n",
        "        The model is able to \"memorize\" the sequence as it goes and utilize the repeated pattern, which is why the loss drops so drastically.\n",
        "        The model‚Äôs predictions are highly dependent on context‚Äîhaving seen the tokens before makes prediction much easier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZbNlFkdG2Z"
      },
      "source": [
        "Attention Head\n",
        "\n",
        "    What it is: An attention head is a component within each layer of a transformer.\n",
        "    What it does: Each head learns to focus (\"pay attention\") to different parts of the input sequence. By doing this, it helps the model process and refine information‚Äîsome heads might focus on nearby tokens, others on far away tokens, etc.\n",
        "    Main point: All attention heads help process and refine information, but each head can learn to specialize in different patterns or relationships.\n",
        "\n",
        "Induction Head\n",
        "\n",
        "    What it is: An induction head is just a special type of attention head.\n",
        "    What it does: Through training, some attention heads learn to specialize in copying or continuing patterns‚Äîif a pattern appears earlier in the sequence, these heads learn to repeat or extend it. This is especially useful for tasks like repeating names or structures.\n",
        "    Main point: Induction heads are not fundamentally different from other attention heads‚Äîthey‚Äôre just attention heads that, through learning, have become good at pattern copying (induction).\n",
        "\n",
        "Relationship Between Them\n",
        "\n",
        "    All induction heads are attention heads, but not all attention heads are induction heads.\n",
        "    Induction heads do not copy patterns from other layers, but from earlier positions within the same input sequence.\n",
        "    Attention heads in higher layers can build on the outputs of lower-layer induction heads, combining pattern copying with other types of processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKR2dKAyh5HM"
      },
      "source": [
        "his code analyzes the behavior of each attention head in every layer of a transformer model, specifically measuring how much each head attends to tokens that are exactly one sequence length earlier‚Äîa hallmark of \"induction heads\", which are responsible for copying or continuing patterns in repeated sequences. The code does this by running the model with hooks that extract and process the attention patterns, computing an average induction score for every head. The results are stored in a tensor and finally visualized as a heatmap, highlighting which heads and layers are most responsible for this induction behavior. This is a powerful diagnostic for understanding model internals and for identifying the heads that learn to \"induce\" or copy patterns in sequences.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwrVseA2ZTXy"
      },
      "outputs": [],
      "source": [
        "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "def induction_score_hook(\n",
        "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
        "    # (This only has entries for tokens with index>=seq_len)\n",
        "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
        "    # Get an average score per head\n",
        "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "    # Store the result.\n",
        "    induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
        "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
        "\n",
        "model.run_with_hooks(\n",
        "    repeated_tokens,\n",
        "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
        "    fwd_hooks=[(\n",
        "        pattern_hook_names_filter,\n",
        "        induction_score_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "imshow(induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nM9f6PImV18"
      },
      "source": [
        "\n",
        "    Y-axis (\"Layer\"): Represents the different layers of the model, with layer numbers increasing from top to bottom.\n",
        "    X-axis (\"Head\"): Represents the different attention heads within each layer.\n",
        "    Color scale (shown on the right): Indicates the induction score for each head. Darker blue colors represent higher induction scores (stronger induction behavior), while lighter shades indicate lower scores. The color bar ranges from negative (red) to positive (blue), but only positive/blue values are prominent here.\n",
        "    Interpretation:\n",
        "        Most of the heads have low induction scores (white/very light blue), meaning they do not strongly exhibit induction behavior.\n",
        "        A few heads (particularly in the middle and deeper layers) have high induction scores (dark blue squares), indicating these heads are specialized for pattern copying or induction.\n",
        "        This pattern helps researchers identify which specific heads and layers in the model are responsible for induction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOomUJOtZTXz"
      },
      "source": [
        "Head 5 in Layer 5 scores extremely highly on this score, and we can feed in a shorter repeated random sequence, visualize the attention pattern for it and see this directly - including the \"induction stripe\" at `seq_len-1` tokens back.\n",
        "\n",
        "This time we put in a hook on the attention pattern activation to visualize the pattern of the relevant head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0WJ3ivZTX0"
      },
      "outputs": [],
      "source": [
        "if IN_GITHUB:\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "induction_head_layer = 5\n",
        "induction_head_index = 5\n",
        "size = (1, 20)\n",
        "input_tensor = torch.randint(1000, 10000, size)\n",
        "\n",
        "single_random_sequence = input_tensor.to(model.cfg.device)\n",
        "repeated_random_sequence = einops.repeat(single_random_sequence, \"batch seq_len -> batch (2 seq_len)\")\n",
        "def visualize_pattern_hook(\n",
        "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    display(\n",
        "        cv.attention.attention_patterns(\n",
        "            tokens=model.to_str_tokens(repeated_random_sequence),\n",
        "            attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.\n",
        "        )\n",
        "    )\n",
        "\n",
        "model.run_with_hooks(\n",
        "    repeated_random_sequence,\n",
        "    return_type=None,\n",
        "    fwd_hooks=[(\n",
        "        utils.get_act_name(\"pattern\", induction_head_layer),\n",
        "        visualize_pattern_hook\n",
        "    )]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo_u3bY8ZTX1"
      },
      "source": [
        "## Available Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHcKOfFHZTX3"
      },
      "source": [
        "TransformerLens comes with over 40 open source models available, all of which can be loaded into a consistent(-ish) architecture by just changing the name in `from_pretrained`. The open source models available are [documented here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h), and a set of interpretability friendly models I've trained are [documented here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m), including a set of toy language models (tiny one to four layer models) and a set of [SoLU models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=FZ5W6GGcy6OitPEaO733JLqf) up to GPT-2 Medium size (300M parameters). You can see [a table of the official alias and hyper-parameters of available models here](https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/model_properties_table.md).\n",
        "\n",
        "**Note:** TransformerLens does not currently support multi-GPU models (which you want for models above eg 7B parameters), but this feature is coming soon!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-35ySdGZTX4"
      },
      "source": [
        "\n",
        "Notably, this means that analysis can be near immediately re-run on a different model by just changing the name - to see this, let's load in DistilGPT-2 (a distilled version of GPT-2, with half as many layers) and copy the code from above to see the induction heads in that model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REzQZlaDZTX5"
      },
      "outputs": [],
      "source": [
        "# NBVAL_IGNORE_OUTPUT\n",
        "distilgpt2 = HookedTransformer.from_pretrained(\"distilgpt2\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT4LgUmBZTX6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "distilgpt2_induction_score_store = torch.zeros((distilgpt2.cfg.n_layers, distilgpt2.cfg.n_heads), device=distilgpt2.cfg.device)\n",
        "def induction_score_hook(\n",
        "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
        "    # (This only has entries for tokens with index>=seq_len)\n",
        "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
        "    # Get an average score per head\n",
        "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "    # Store the result.\n",
        "    distilgpt2_induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
        "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
        "\n",
        "distilgpt2.run_with_hooks(\n",
        "    repeated_tokens,\n",
        "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
        "    fwd_hooks=[(\n",
        "        pattern_hook_names_filter,\n",
        "        induction_score_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "imshow(distilgpt2_induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head in Distil GPT-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JR27SxsZTX7"
      },
      "source": [
        "\n",
        "### An overview of the important open source models in the library\n",
        "\n",
        "GPT-2 (OpenAI):\n",
        "Classic generative model with sizes: 85M, 300M, 700M, and 1.5B parameters.\n",
        "Trained on ~22B tokens of internet text.\n",
        "Open source dataset\n",
        "\n",
        "GPT-Neo (EleutherAI):\n",
        "GPT-2-style models with 125M, 1.3B, and 2.7B parameters.\n",
        "Trained on ~300B tokens from The Pile (a large, diverse dataset).\n",
        "\n",
        "OPT (Meta):\n",
        "Models with 125M to 66B parameters, trained on 180B tokens of varied text.\n",
        "\n",
        "GPT-J (EleutherAI):\n",
        "6B parameter model trained on The Pile.\n",
        "\n",
        "GPT-NeoX (EleutherAI):\n",
        "20B parameter model, also trained on The Pile.\n",
        "\n",
        "StableLM (Stability AI):\n",
        "3B and 7B models, with versions for chat and instruction tasks.\n",
        "\n",
        "Stanford CRFM Models:\n",
        "Replications of GPT-2 Small and Medium, each trained with 5 random seeds.\n",
        "600 checkpoints available per model, accessible via e.g.\n",
        "HookedTransformer.from_pretrained(\"stanford-gpt2-small-a\", checkpoint_index=265).\n",
        "\n",
        "BERT (Google):\n",
        "Encoder-only model (108M parameters).\n",
        "Trained on Wikipedia and BooksCorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTccPfQnZTX8"
      },
      "source": [
        "\n",
        "### An overview of some interpretability-friendly models I've trained and included\n",
        "\n",
        "(Feel free to [reach out](mailto:neelnanda27@gmail.com) if you want more details on any of these models)\n",
        "\n",
        "Each of these models has about ~200 checkpoints taken during training that can also be loaded from TransformerLens, with the `checkpoint_index` argument to `from_pretrained`.\n",
        "\n",
        "Note that all models are trained with a Beginning of Sequence token, and will likely break if given inputs without that!\n",
        "\n",
        "* **Toy Models**: Inspired by [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), I've trained 12 tiny language models, of 1-4L and each of width 512. I think that interpreting these is likely to be far more tractable than larger models, and both serve as good practice and will likely contain motifs and circuits that generalise to far larger models (like induction heads):\n",
        "    * Attention-Only models (ie without MLPs): attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l\n",
        "    * GELU models (ie with MLP, and the standard GELU activations): gelu-1l, gelu-2l, gelu-3l, gelu-4l\n",
        "    * SoLU models (ie with MLP, and [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), designed to make MLP neurons more interpretable): solu-1l, solu-2l, solu-3l, solu-4l\n",
        "    * All models are trained on 22B tokens of data, 80% from C4 (web text) and 20% from Python Code\n",
        "    * Models of the same layer size were trained with the same weight initialization and data shuffle, to more directly compare the effect of different activation functions.\n",
        "* **SoLU** models: A larger scan of models trained with [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), in the hopes that it makes the MLP neuron interpretability easier.\n",
        "    * A scan up to GPT-2 Medium size, trained on 30B tokens of the same data as toy models, 80% from C4 and 20% from Python code.\n",
        "        * solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)\n",
        "    * An older scan up to GPT-2 Medium size, trained on 15B tokens of [the Pile](https://pile.eleuther.ai/)\n",
        "        * solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M), solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile (200M), solu-12l-pile (340M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtHAKyMcBppa"
      },
      "source": [
        "Text simplified:\n",
        "Interpretability-Friendly Models I've Trained\n",
        "\n",
        "(Feel free to reach out if you want more details!)\n",
        "\n",
        "    Checkpoints:\n",
        "    Each model includes ~200 training checkpoints, loadable via checkpoint_index in from_pretrained.\n",
        "\n",
        "    Important Note:\n",
        "    All models require a Beginning of Sequence (BOS) token ‚Äî they may break without it.\n",
        "\n",
        "Toy Models\n",
        "\n",
        "Small, interpretable models (1‚Äì4 layers, width 512), trained on 22B tokens (80% C4, 20% Python).\n",
        "Same layer-size models share initialization and data shuffle for fair comparison.\n",
        "\n",
        "    Attention-Only (no MLP):\n",
        "    attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l\n",
        "\n",
        "    GELU (standard MLPs):\n",
        "    gelu-1l, gelu-2l, gelu-3l, gelu-4l\n",
        "\n",
        "    SoLU (Anthropic‚Äôs interpretable MLP activations):\n",
        "    solu-1l, solu-2l, solu-3l, solu-4l\n",
        "\n",
        "These models are great for learning interpretability techniques and may reveal patterns seen in larger models (e.g. induction heads).\n",
        "SoLU Scan Models\n",
        "\n",
        "Trained on 30B tokens (same mix as toy models):\n",
        "\n",
        "    solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)\n",
        "\n",
        "Older scan, trained on 15B tokens of The Pile:\n",
        "\n",
        "    solu-1l-pile to solu-12l-pile (sizes: 13M‚Äì340M)\n",
        "\n",
        "These are aimed at improving MLP neuron interpretability using SoLU activations.\n",
        "\n",
        "\n",
        "These toy models are designed to help researchers understand and interpret the internal mechanisms of large language models (LLMs), but in a much simpler setting.\n",
        "Why they‚Äôre useful for interpretation:\n",
        "\n",
        "    Smaller size (1‚Äì4 layers, 512 width) ‚Üí Easier to visualize and trace computations.\n",
        "\n",
        "    Simplified structure (e.g., attention-only) ‚Üí Focus on specific components like attention heads.\n",
        "\n",
        "    Controlled training setup ‚Üí All models use the same data, initialization, and training procedure, so differences are easier to attribute (e.g., to activation functions).\n",
        "\n",
        "    Known dataset (22B tokens from C4 and code) ‚Üí Makes analysis more grounded.\n",
        "\n",
        "What they help interpret:\n",
        "\n",
        "    Attention patterns ‚Äì like induction heads, copying, positional reasoning.\n",
        "\n",
        "    Activation functions ‚Äì comparing GELU vs SoLU and how they affect neuron behavior.\n",
        "\n",
        "    Circuit motifs ‚Äì reusable patterns (e.g., name matching, list continuation) that also appear in large LLMs.\n",
        "\n",
        "So yes, these toy models act like simplified microscopes for studying the complex systems inside big models like GPT-2 or GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXFqbxPZTX8"
      },
      "source": [
        "## Other Resources:\n",
        "\n",
        "* [Concrete Steps to Get Started in Mechanistic Interpretability](https://neelnanda.io/getting-started): A guide I wrote for how to get involved in mechanistic interpretability, and how to learn the basic skills\n",
        "* [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): An overview of concepts in the field and surrounding ideas in ML and transformers, with long digressions to give context and build intuitions.\n",
        "* [Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems), a doc I wrote giving a long list of open problems in mechanistic interpretability, and thoughts on how to get started on trying to work on them.\n",
        "    * There's a lot of low-hanging fruit in the field, and I expect that many people reading this could use TransformerLens to usefully make progress on some of these!\n",
        "* Other demos:\n",
        "    * **[Exploratory Analysis Demo](https://neelnanda.io/exploratory-analysis-demo)**, a demonstration of my standard toolkit for how to use TransformerLens to explore a mysterious behaviour in a language model.\n",
        "    * [Interpretability in the Wild](https://github.com/redwoodresearch/Easy-Transformer) a codebase from Arthur Conmy and Alex Variengien at Redwood research using this library to do a detailed and rigorous reverse engineering of the Indirect Object Identification circuit, to accompany their paper\n",
        "        * Note - this was based on an earlier version of this library, called EasyTransformer. It's pretty similar, but several breaking changes have been made since.\n",
        "    * A [recorded walkthrough](https://www.youtube.com/watch?v=yo4QvDn-vsU) of me doing research with TransformerLens on whether a tiny model can re-derive positional information, with [an accompanying Colab](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/No_Position_Experiment.ipynb)\n",
        "* [Neuroscope](https://neuroscope.io), a website showing the text in the dataset that most activates each neuron in some selected models. Good to explore to get a sense for what kind of features the model tends to represent, and as a \"wiki\" to get some info\n",
        "    * A tutorial on how to make an [Interactive Neuroscope](https://github.com/TransformerLensOrg/TransformerLens/blob/main/Hacky-Interactive-Lexoscope.ipynb), where you type in text and see the neuron activations over the text update live."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATpYucpWZTX8"
      },
      "source": [
        "## Transformer architecture\n",
        "\n",
        "HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads:\n",
        "* The weights (W_K, W_Q, W_V) mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
        "* The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys, queries, values, z (values mixed by attention pattern)) have separate head_index and d_head axes, rather than flattening them into one big axis.\n",
        "    * The activations all have shape `[batch, position, head_index, d_head]`\n",
        "    * W_K, W_Q, W_V have shape `[head_index, d_model, d_head]` and W_O has shape `[head_index, d_head, d_model]`\n",
        "\n",
        "The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR2HtsA4GnCo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUVz7atHLmJm"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-GMV90kLmYa"
      },
      "source": [
        "Simplified text:\n",
        "üè≠ In HookedTransformer:\n",
        "\n",
        "    Instead of one big machine, the factory uses three separate, specialized machines for sorting (W_K), labeling (W_Q), and inspecting (W_V).\n",
        "    Each has its own clear job and design.\n",
        "\n",
        "üß† Head Structure:\n",
        "\n",
        "    In GPT-2, the tools inside each machine are all mixed together‚Äîlike if one long workbench had different tools scattered without clear boundaries between teams (heads).\n",
        "\n",
        "    In HookedTransformer, the tools are organized by team (head_index) and tool type (d_head).\n",
        "    Imagine rows of benches‚Äîone per team‚Äîand each team has their own neatly arranged set of tools.\n",
        "\n",
        "üì¶ Data Flow Shapes:\n",
        "\n",
        "    The packages (data activations) are clearly labeled like:\n",
        "    [batch, position, head_index, d_head]\n",
        "    (i.e., which package, where in the line, which team, and which part of the toolset is used)\n",
        "\n",
        "    The machines (weights) also follow this organized structure:\n",
        "\n",
        "        Each team has their own machine settings (weights), no sharing between teams.\n",
        "\n",
        "        The machines are built like:\n",
        "\n",
        "            Sorting, labeling, inspecting tools: [head_index, d_model, d_head]\n",
        "\n",
        "            Final assembly tool (W_O): [head_index, d_head, d_model]\n",
        "\n",
        "üõ†Ô∏è Code Reality:\n",
        "\n",
        "    The factory control system (code) is a bit messy‚Äîlots of switches and toggles (Boolean flags) to handle different factory designs (model families)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dgn8igYLGi"
      },
      "source": [
        "üõ†Ô∏è Specialized Machines (Weights):\n",
        "\n",
        "Each attention head (team) has its own set of tools (weight matrices) for:\n",
        "\n",
        "    Sorting ‚Üí Keys ‚Üí W_K\n",
        "\n",
        "    Labeling ‚Üí Queries ‚Üí W_Q\n",
        "\n",
        "    Inspecting ‚Üí Values ‚Üí W_V\n",
        "\n",
        "These tools are shaped as:\n",
        "\n",
        "    [head_index, d_model, d_head]\n",
        "\n",
        "    head_index: Which team (attention head) it belongs to\n",
        "\n",
        "    d_model: Size of the full input (the entire package)\n",
        "\n",
        "    d_head: Size of the smaller chunk each team works with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q5VM86KZTX8"
      },
      "source": [
        "### Parameter Names\n",
        "\n",
        "Here is a list of the parameters and shapes in the model. By convention, all weight matrices multiply on the right (ie `new_activation = old_activation @ weights + bias`).\n",
        "\n",
        "Reminder of the key hyper-params:\n",
        "* `n_layers`: 12. The number of transformer blocks in the model (a block contains an attention layer and an MLP layer)\n",
        "* `n_heads`: 12. The number of attention heads per attention layer\n",
        "* `d_model`: 768. The residual stream width.\n",
        "* `d_head`: 64. The internal dimension of an attention head activation.\n",
        "* `d_mlp`: 3072. The internal dimension of the MLP layers (ie the number of neurons).\n",
        "* `d_vocab`: 50267. The number of tokens in the vocabulary.\n",
        "* `n_ctx`: 1024. The maximum number of tokens in an input prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQwXr8EZcqx"
      },
      "source": [
        "üè≠ The Factory = The Transformer Model\n",
        "\n",
        "You can imagine your transformer as a factory that processes text input (like a sentence) and produces smart predictions (like the next word).\n",
        "‚öôÔ∏è The Main Parts of the Factory\n",
        "\n",
        "Each sentence goes through a 12-stage assembly line (that‚Äôs n_layers = 12).\n",
        "Each stage is a transformer block, and it has two parts:\n",
        "\n",
        "    üß† Attention Unit ‚Äì helps figure out which words in the sentence are important to each other.\n",
        "\n",
        "    üí™ MLP Unit ‚Äì applies deeper thinking or transformations to each word separately.\n",
        "\n",
        "üóÉÔ∏è Key Components in the Factory\n",
        "üß≥ The Input Bags = Tokens\n",
        "\n",
        "Each word or part of a word is turned into a number from a vocabulary of 50,267 items (d_vocab).\n",
        "üßµ The Conveyor Belt = Residual Stream\n",
        "\n",
        "Each word is turned into a vector of size 768 (d_model), and these flow through the factory like items on a conveyor belt. Everything is built around this stream.\n",
        "üß≠ Positional Guide = Positional Embedding\n",
        "\n",
        "Since word order matters (\"dog bites man\" ‚â† \"man bites dog\"), we give each position on the conveyor belt a label to help the model understand order.\n",
        "üß† Attention Unit = Word Relationship Finder\n",
        "\n",
        "At each layer:\n",
        "\n",
        "    There are 12 little inspectors (n_heads = 12), each focusing on a different pattern (like checking rhymes, grammar, or logic).\n",
        "\n",
        "    Each inspector looks at all the other words to decide which ones are important.\n",
        "\n",
        "    Each does this using:\n",
        "\n",
        "        Q (question it asks),\n",
        "\n",
        "        K (how relevant other words are),\n",
        "\n",
        "        V (what info those words carry).\n",
        "\n",
        "    Their thinking happens in 64 dimensions (d_head = 64), and they all report back to update the word representations.\n",
        "\n",
        "üí™ MLP Unit = Deep Thinking Machine\n",
        "\n",
        "After attention, each word gets passed through a mini-brain (the MLP):\n",
        "\n",
        "    Expands the word‚Äôs vector from 768 ‚Üí 3072 (d_mlp) ‚Äî like opening up for deep thought.\n",
        "\n",
        "    Then compresses it back to 768.\n",
        "\n",
        "    Activation functions (like GELU or SoLU) help add nonlinear thinking ‚Äî like giving intuition or emotion.\n",
        "\n",
        "üì¶ Embeddings and Output\n",
        "\n",
        "    At the start:\n",
        "    Words are turned into vectors using an embedding table: (50,267 √ó 768).\n",
        "\n",
        "    At the end:\n",
        "    The model guesses the next word by mapping back from 768 ‚Üí 50,267 possibilities using the unembedding matrix.\n",
        "\n",
        "üîÅ Checkpoints = Snapshots of the Factory\n",
        "\n",
        "You have ~200 snapshots (checkpoints) of the factory while it was being built ‚Äî like checking its progress during training.\n",
        "üö® Important Rule:\n",
        "\n",
        "Every input must begin with a \"Start of Sequence\" token ‚Äî it tells the factory, ‚ÄúHey! A new job is starting!‚Äù\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zOEww6GZTX-"
      },
      "source": [
        "**Transformer Block parameters:**\n",
        "Replace 0 with the relevant layer index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpWFG9DTZTX-"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"blocks.0.\"):\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY5n1_DQ-lhA"
      },
      "source": [
        " Summary Table\n",
        "Parameter\tShape\tMeaning\n",
        "W_Q / W_K / W_V\t[12, 768, 64]\tPer-head projection matrices for Queries, Keys, and Values\n",
        "b_Q / b_K / b_V\t[12, 64]\tPer-head biases for Q, K, V\n",
        "W_O\t[12, 64, 768]\tPer-head output projection\n",
        "b_O\t[768]\tFinal attention output bias\n",
        "W_in / b_in\t[768, 3072] / [3072]\tInput-to-hidden weights and bias for MLP\n",
        "W_out / b_out\t[3072, 768] / [768]\tHidden-to-output weights and bias for MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhF5oaP7LoA"
      },
      "source": [
        "üß† Summary: What Happens in blocks.0\n",
        "\n",
        "    Input is normalized (ln1)\n",
        "\n",
        "    Goes through multi-head attention:\n",
        "\n",
        "        Projected to Q/K/V\n",
        "\n",
        "        Attention calculated\n",
        "\n",
        "        Projected back with W_o\n",
        "\n",
        "    Output added back (residual)\n",
        "\n",
        "    Result is normalized again (ln2)\n",
        "\n",
        "    Goes through MLP:\n",
        "\n",
        "        Projected up (W_in)\n",
        "\n",
        "        Activated\n",
        "\n",
        "        Projected down (W_out)\n",
        "\n",
        "    Output added back again (residual)\n",
        "\n",
        "This same pattern repeats for all 12 blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otm-fh3pmOrZ"
      },
      "source": [
        "Begin from here **bold text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSfCimYl7Cdn"
      },
      "source": [
        "Absolutely! Let's walk through what happens in **Block 0** of a transformer using a **simple text input**, with all the main steps explained clearly.\n",
        "\n",
        "---\n",
        "\n",
        "### üìù Example Input:\n",
        "\n",
        "Let‚Äôs say we input the sentence:\n",
        "\n",
        "```\n",
        "\"The cat sat\"\n",
        "```\n",
        "\n",
        "We'll assume it's already tokenized into token IDs and embedded.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Model Setup:\n",
        "\n",
        "* `d_model = 768` (each token is a 768-dimensional vector)\n",
        "* `n_heads = 12` (each head handles 64 dimensions)\n",
        "* Block 0 includes:\n",
        "\n",
        "  * LayerNorm ‚Üí Attention ‚Üí Residual\n",
        "  * LayerNorm ‚Üí MLP ‚Üí Residual\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Step-by-Step Through **Block 0**:\n",
        "\n",
        "#### 1Ô∏è‚É£ **Embedding + Positional Encoding**\n",
        "\n",
        "Each token like `\"The\"`, `\"cat\"`, and `\"sat\"` is converted into a vector of shape `[768]` and combined with a positional vector (so it knows the order).\n",
        "So your sequence looks like:\n",
        "\n",
        "```python\n",
        "x = [\n",
        "  [embedding of \"The\" + position 0],   # shape: (768)\n",
        "  [embedding of \"cat\" + position 1],\n",
        "  [embedding of \"sat\" + position 2]\n",
        "]  ‚Üí Shape: (3 tokens √ó 768 dims)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2Ô∏è‚É£ **LayerNorm 1**\n",
        "\n",
        "Each 768-dimensional vector is **normalized** to make learning more stable.\n",
        "\n",
        "```python\n",
        "x_norm = LayerNorm(x)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3Ô∏è‚É£ **Compute Q, K, V**\n",
        "\n",
        "We now generate **Query (Q)**, **Key (K)**, and **Value (V)** vectors:\n",
        "\n",
        "```python\n",
        "Q = x_norm @ W_q   # shape: (3, 768)\n",
        "K = x_norm @ W_k   # shape: (3, 768)\n",
        "V = x_norm @ W_v   # shape: (3, 768)\n",
        "```\n",
        "\n",
        "Each of these vectors is then split into 12 heads (64 dimensions each), so for each token you now have:\n",
        "\n",
        "```python\n",
        "Q_heads = reshape(Q, (3, 12, 64))   # (tokens, heads, head_dim)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 4Ô∏è‚É£ **Scaled Dot-Product Attention**\n",
        "\n",
        "For each head, calculate:\n",
        "\n",
        "```python\n",
        "Attention_scores = Q ‚Ä¢ K^T / sqrt(64)  # shape: (3, 3) per head\n",
        "Attention_probs = softmax(Attention_scores)\n",
        "Head_output = Attention_probs @ V      # shape: (3, 64) per head\n",
        "```\n",
        "\n",
        "Then concatenate all heads back:\n",
        "\n",
        "```python\n",
        "combined_heads = concat(Head_output)  # shape: (3, 768)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 5Ô∏è‚É£ **Output Projection (W\\_o)**\n",
        "\n",
        "```python\n",
        "attention_output = combined_heads @ W_o  # shape: (3, 768)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 6Ô∏è‚É£ **Residual Connection**\n",
        "\n",
        "Add the attention output back to the input:\n",
        "\n",
        "```python\n",
        "x = x + attention_output\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 7Ô∏è‚É£ **LayerNorm 2**\n",
        "\n",
        "Normalize the result again:\n",
        "\n",
        "```python\n",
        "x_norm = LayerNorm(x)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 8Ô∏è‚É£ **MLP: Feedforward Layer**\n",
        "\n",
        "Apply the MLP in two steps:\n",
        "\n",
        "```python\n",
        "hidden = x_norm @ W_in         # shape: (3, 3072)\n",
        "activated = GELU(hidden)       # shape: (3, 3072)\n",
        "mlp_output = activated @ W_out # shape: (3, 768)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 9Ô∏è‚É£ **Final Residual Add**\n",
        "\n",
        "```python\n",
        "x = x + mlp_output\n",
        "```\n",
        "\n",
        "Now the tokens carry updated representations ‚Äî enriched with attention context and non-linear transformations.\n",
        "\n",
        "---\n",
        "\n",
        "### üîö Final Output of Block 0:\n",
        "\n",
        "Shape: `(3 tokens, 768)`\n",
        "Each token‚Äôs vector now contains information **from other tokens** and **from deeper processing**.\n",
        "\n",
        "For example:\n",
        "\n",
        "* The vector for `\"cat\"` now knows it comes **after** \"The\" and **before** \"sat\".\n",
        "* The vector for `\"sat\"` might start to understand it‚Äôs a **verb related to a subject** (\"cat\").\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you‚Äôd like to visualize this with tensors or build it in code step-by-step!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-1j-VmUZTX-"
      },
      "source": [
        "**Embedding & Unembedding parameters:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N_QffOtZTX_"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"blocks\"):\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjKp9W_P_qd3"
      },
      "source": [
        "This code will show you:\n",
        "    Anything else outside \"blocks.*\" (like auxiliary heads or metadata)\n",
        "\n",
        "    The embedding layers\n",
        "\n",
        "    Possibly the final unembedding layer\n",
        "\n",
        "    Maybe a final LayerNorm\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87HQgXjm_1dH"
      },
      "source": [
        "Here‚Äôs what those parameters mean in simple terms:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **embed.W\\_E** ‚Äî Token Embeddings\n",
        "\n",
        "* Shape: `[50257, 768]`\n",
        "* This is a big lookup table. Each of the 50,257 tokens in your vocabulary gets represented as a 768-dimensional vector before going into the transformer blocks.\n",
        "* Think: turning words/tokens into numbers the model can understand.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **pos\\_embed.W\\_pos** ‚Äî Positional Embeddings\n",
        "\n",
        "* Shape: `[1024, 768]`\n",
        "* Adds information about the position of each token in the sequence (up to 1024 tokens).\n",
        "* Helps the model know the order of tokens because transformers don‚Äôt have built-in order awareness.\n",
        "* Think: ‚Äútoken #1,‚Äù ‚Äútoken #2,‚Äù etc., encoded as vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **unembed.W\\_U** ‚Äî Unembedding Weights\n",
        "\n",
        "* Shape: `[768, 50257]`\n",
        "* Maps the model‚Äôs final 768-dimensional hidden vectors back to the vocabulary space ‚Äî basically turning model output into logits for each token.\n",
        "* Think: turning the model‚Äôs internal understanding back into actual words.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **unembed.b\\_U** ‚Äî Unembedding Bias\n",
        "\n",
        "* Shape: `[50257]`\n",
        "* A bias added to the logits before predicting the next token, helps adjust the prediction distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Flow:\n",
        "\n",
        "**Input tokens ‚Üí embed.W\\_E ‚Üí add pos\\_embed.W\\_pos ‚Üí Transformer blocks ‚Üí unembed.W\\_U + unembed.b\\_U ‚Üí Predicted next token probabilities**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "455iUYsjDsyD"
      },
      "source": [
        "The whole process of transformer model learning\n",
        "Exactly! Here‚Äôs a simple step-by-step of what happens inside **Block 0** and after:\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: **Attention calculation**\n",
        "\n",
        "* The input vectors (after embedding + position encoding) enter Block 0.\n",
        "* For each attention head, the model uses the **weights (`W_Q`, `W_K`, `W_V`) and biases (`b_Q`, `b_K`, `b_V`)** to compute Query, Key, and Value vectors.\n",
        "* These are used to calculate **attention scores** ‚Äî basically, how much each token should focus on every other token.\n",
        "* Attention scores are normalized (softmax), multiplied by the Value vectors, then combined across heads.\n",
        "* The combined result is multiplied by the **output projection matrix `W_O` and bias `b_O`** to produce the attention output.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: **Feed the attention output to the MLP**\n",
        "\n",
        "* The attention output goes into the **MLP** (multi-layer perceptron):\n",
        "\n",
        "  * First transformed by `W_in` + `b_in`\n",
        "  * Activation function (like GELU or SoLU) applied\n",
        "  * Then transformed back by `W_out` + `b_out`\n",
        "* This MLP adds more complexity and nonlinear transformation to each token‚Äôs vector.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: **Residual connections and layer norms**\n",
        "\n",
        "* Typically, the block also adds **residual connections** (adding input back to output at each stage) and applies **layer normalization** to stabilize training.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: **After all transformer blocks**\n",
        "\n",
        "* The output from the final block goes to the **unembedding layer (`W_U` + `b_U`)** which maps vectors back to vocabulary logits.\n",
        "* These logits are used to predict the next token.\n",
        "\n",
        "---\n",
        "\n",
        "### So, in short:\n",
        "\n",
        "```plaintext\n",
        "Input embeddings\n",
        "   ‚Üì\n",
        "Block 0 Attention (Q, K, V weights + biases) ‚Üí attention output\n",
        "   ‚Üì\n",
        "MLP (weights + biases + activation)\n",
        "   ‚Üì\n",
        "Residual + Norm\n",
        "   ‚Üì\n",
        "Next blocks‚Ä¶\n",
        "   ‚Üì\n",
        "Final unembedding ‚Üí token prediction\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "You got it perfectly! Want me to show code snippets or a diagram for this?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mp7lcKAimmTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_JUSrSFZTX_"
      },
      "source": [
        "### Activation + Hook Names\n",
        "\n",
        "Lets get out a list of the activation/hook names in the model and their shapes. In practice, I recommend using the `utils.get_act_name` function to get the names, but this is a useful fallback, and necessary to eg write a name filter function.\n",
        "\n",
        "Let's do this by entering in a short, 10 token prompt, and add a hook function to each activations to print its name and shape. To avoid spam, let's just add this to activations in the first block or not in a block.\n",
        "\n",
        "Note 1: Each LayerNorm has a hook for the scale factor (ie the standard deviation of the input activations for each token position & batch element) and for the normalized output (ie the input activation with mean 0 and standard deviation 1, but *before* applying scaling or translating with learned weights). LayerNorm is applied every time a layer reads from the residual stream: `ln1` is the LayerNorm before the attention layer in a block, `ln2` the one before the MLP layer, and `ln_final` is the LayerNorm before the unembed.\n",
        "\n",
        "Note 2: *Every* activation apart from the attention pattern and attention scores has shape beginning with `[batch, position]`. The attention pattern and scores have shape `[batch, head_index, dest_position, source_position]` (the numbers are the same, unless we're using caching)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gra8mg3PIM0V"
      },
      "source": [
        "Simplified text:\n",
        "Sure! Here's a **simplified version** of that explanation:\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Activation + Hook Names (Simplified)\n",
        "\n",
        "We can print out the **names and shapes of activations** in the model by using a short prompt and attaching **hooks** to them.\n",
        "\n",
        "* You can normally use `utils.get_act_name()` to get activation names.\n",
        "* But it‚Äôs also useful to manually check names/shapes ‚Äî especially if you're writing a function that filters by name.\n",
        "\n",
        "To avoid too much output, we‚Äôll just focus on:\n",
        "\n",
        "* Activations **in the first block**\n",
        "* Or activations **outside the blocks** (like embedding, final layer norm, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ÑπÔ∏è Notes:\n",
        "\n",
        "**1. LayerNorm Hooks:**\n",
        "\n",
        "* Each LayerNorm gives two things:\n",
        "\n",
        "  * **Scale (std deviation)**: shows how much the input varies\n",
        "  * **Normalized output**: input adjusted to have mean 0, std 1 (before scaling/offset)\n",
        "\n",
        "LayerNorm locations:\n",
        "\n",
        "* `ln1`: before attention layer\n",
        "* `ln2`: before MLP\n",
        "* `ln_final`: before unembedding\n",
        "\n",
        "**2. Activation Shapes:**\n",
        "\n",
        "* Most activations: `[batch, position, ...]`\n",
        "* Attention patterns/scores: `[batch, head, dest_pos, src_pos]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnWu1wvsM01x"
      },
      "source": [
        "More simplified;\n",
        "üîç Summary: Printing Activation Names and Shapes in a Transformer Model\n",
        "\n",
        "    Prepare a short input prompt:\n",
        "    You use the sentence \"The quick brown fox jumped over the lazy dog\" and tokenize it to count how many tokens will be processed.\n",
        "\n",
        "    Define a hook function:\n",
        "    A custom function (print_name_shape_hook_function) is created to print the name and shape of any activation it‚Äôs attached to.\n",
        "\n",
        "    Set up a filter:\n",
        "    You use a filter to apply hooks only to:\n",
        "\n",
        "        Layers inside the first transformer block (blocks.0)\n",
        "\n",
        "        Layers outside all blocks (like embeddings and final layer norm)\n",
        "\n",
        "    Run the model with hooks:\n",
        "    The model processes the prompt using run_with_hooks. As it goes through the selected layers, your hook prints the name and shape of each activation.\n",
        "\n",
        "This helps you inspect and understand how data flows through the early parts of the transformer during inference, which is especially useful for interpretability and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHLCQ1HkZTYA"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"The quick brown fox jumped over the lazy dog\"\n",
        "print(\"Num tokens:\", len(model.to_tokens(test_prompt)[0]))\n",
        "\n",
        "def print_name_shape_hook_function(activation, hook):\n",
        "    print(hook.name, activation.shape)\n",
        "\n",
        "not_in_late_block_filter = lambda name: name.startswith(\"blocks.0.\") or not name.startswith(\"blocks\")\n",
        "\n",
        "model.run_with_hooks(\n",
        "    test_prompt,\n",
        "    return_type=None,\n",
        "    fwd_hooks=[(not_in_late_block_filter, print_name_shape_hook_function)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl7LruDdR0aT"
      },
      "source": [
        "üß† Transformer Forward Pass (Block 0 Only)\n",
        "\n",
        "You input a 10-token sentence, and the model processes it through the following steps:\n",
        "üîπ Embedding Layers\n",
        "\n",
        "    hook_embed: Word token embeddings ‚Üí shape [1, 10, 768]\n",
        "\n",
        "    hook_pos_embed: Positional embeddings ‚Üí shape [1, 10, 768]\n",
        "\n",
        "    These are added together to start the transformer processing.\n",
        "\n",
        "üîπ Block 0 Begins\n",
        "1. Before Attention\n",
        "\n",
        "    blocks.0.hook_resid_pre: The input to Block 0 from the residual stream.\n",
        "\n",
        "    blocks.0.ln1.hook_scale & hook_normalized: LayerNorm is applied to normalize inputs before attention.\n",
        "\n",
        "2. Attention Mechanism\n",
        "\n",
        "    hook_q, hook_k, hook_v: Query, Key, Value vectors for all 12 heads ‚Üí shape [1, 10, 12, 64]\n",
        "\n",
        "    hook_attn_scores: Dot products between queries and keys ‚Üí [1, 12, 10, 10]\n",
        "\n",
        "    hook_pattern: Softmax of scores = attention weights (how much each token attends to others)\n",
        "\n",
        "    hook_z: The result after applying attention weights to values ‚Üí per head output\n",
        "\n",
        "    hook_attn_out: Heads are combined and linearly projected ‚Üí full attention output [1, 10, 768]\n",
        "\n",
        "3. Mid Residual\n",
        "\n",
        "    hook_resid_mid: Residual stream after adding attention output.\n",
        "\n",
        "4. MLP Path\n",
        "\n",
        "    ln2.hook_scale & hook_normalized: Second LayerNorm before MLP\n",
        "\n",
        "    mlp.hook_pre: Output of MLP's first linear layer ‚Üí shape [1, 10, 3072]\n",
        "\n",
        "    mlp.hook_post: After activation function (e.g. GELU or SoLU)\n",
        "\n",
        "    hook_mlp_out: Final output of MLP ‚Üí mapped back to residual stream shape [1, 10, 768]\n",
        "\n",
        "5. After Block 0\n",
        "\n",
        "    hook_resid_post: Final output from Block 0 (after MLP and residual connection)\n",
        "\n",
        "üîπ Final LayerNorm\n",
        "\n",
        "    ln_final.hook_scale & hook_normalized: Prepares the data for unembedding and token prediction\n",
        "\n",
        "‚úÖ Summary:\n",
        "\n",
        "You‚Äôre seeing the complete flow of data through Block 0, including:\n",
        "\n",
        "    Embedding ‚Üí Attention ‚Üí MLP\n",
        "\n",
        "    Each step‚Äôs intermediate activations and transformations\n",
        "\n",
        "    How the model tracks and modifies token representations through residual streams and layer norms\n",
        "\n",
        "This is extremely useful for debugging and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMw2QrHnZTYA"
      },
      "source": [
        "### Folding LayerNorm (For the Curious)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FcDyuxsZTYA"
      },
      "source": [
        "(For the curious - this is an important technical detail that's worth understanding, especially if you have preconceptions about how transformers work, but not necessary to use TransformerLens)\n",
        "\n",
        "LayerNorm is a normalization technique used by transformers, analogous to BatchNorm but more friendly to massive parallelisation. No one *really* knows why it works, but it seems to improve model numerical stability. Unlike BatchNorm, LayerNorm actually changes the functional form of the model, which makes it a massive pain for interpretability!\n",
        "\n",
        "Folding LayerNorm is a technique to make it lower overhead to deal with, and the flags `center_writing_weights` and `fold_ln` in `HookedTransformer.from_pretrained` apply this automatically (they default to True). These simplify the internal structure without changing the weights.\n",
        "\n",
        "Intuitively, LayerNorm acts on each residual stream vector (ie for each batch element and token position) independently, sets their mean to 0 (centering) and standard deviation to 1 (normalizing) (*across* the residual stream dimension - very weird!), and then applies a learned elementwise scaling and translation to each vector.\n",
        "\n",
        "Mathematically, centering is a linear map, normalizing is *not* a linear map, and scaling and translation are linear maps.\n",
        "* **Centering:** LayerNorm is applied every time a layer reads from the residual stream, so the mean of any residual stream vector can never matter - `center_writing_weights` set every weight matrix writing to the residual to have zero mean.\n",
        "* **Normalizing:** Normalizing is not a linear map, and cannot be factored out. The `hook_scale` hook point lets you access and control for this.\n",
        "* **Scaling and Translation:** Scaling and translation are linear maps, and are always followed by another linear map. The composition of two linear maps is another linear map, so we can *fold* the scaling and translation weights into the weights of the subsequent layer, and simplify things without changing the underlying computation.\n",
        "\n",
        "[See the docs for more details](https://github.com/TransformerLensOrg/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMM96_0QZTYB"
      },
      "source": [
        "A fun consequence of LayerNorm folding is that it creates a bias across the unembed, a `d_vocab` length vector that is added to the output logits - GPT-2 is not trained with this, but it *is* trained with a final LayerNorm that contains a bias.\n",
        "\n",
        "Turns out, this LayerNorm bias learns structure of the data that we can only see after folding! In particular, it essentially learns **unigram statistics** - rare tokens get suppressed, common tokens get boosted, by pretty dramatic degrees! Let's list the top and bottom 20 - at the top we see common punctuation and words like \" the\" and \" and\", at the bottom we see weird-ass tokens like \" RandomRedditor\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csn2Nqi-aInX"
      },
      "source": [
        "Simplified:\n",
        "Absolutely ‚Äî let‚Äôs walk through **how Folding LayerNorm works**, using a **simple text input** and step-by-step explanation.\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Example Input\n",
        "\n",
        "Let‚Äôs take a short prompt:\n",
        "\n",
        "```python\n",
        "prompt = \"Hello world\"\n",
        "```\n",
        "\n",
        "Suppose this gets tokenized into 2 tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ What Normally Happens (No Folding)\n",
        "\n",
        "1. **Embedding**\n",
        "   Each token becomes a vector of size `d_model = 768`.\n",
        "\n",
        "2. **Residual Stream**\n",
        "   This is the main data path in a transformer. It stores the token representations as they get updated by each layer.\n",
        "\n",
        "3. **LayerNorm (ln1)**\n",
        "   Before passing data to **attention**, a LayerNorm is applied:\n",
        "\n",
        "   * Each token‚Äôs vector (in the residual stream) is **centered** (mean = 0)\n",
        "   * Then **normalized** (std = 1)\n",
        "   * Then **scaled and shifted** using learned weights\n",
        "\n",
        "4. **Attention ‚Üí Output added back to Residual Stream**\n",
        "\n",
        "5. **Another LayerNorm (ln2)** before the **MLP**, same process\n",
        "\n",
        "6. **Final LayerNorm** before unembedding (getting logits)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ With LayerNorm Folding\n",
        "\n",
        "Now let‚Äôs apply **LayerNorm folding** to simplify this:\n",
        "\n",
        "#### Folding Step 1: **Centering**\n",
        "\n",
        "We modify any **weight matrix that writes to the residual stream**, so its **output will always have mean zero**.\n",
        "‚Üí This means LayerNorm‚Äôs centering step is now *built into the weight* ‚Äî no need to center again later.\n",
        "\n",
        "#### Folding Step 2: **Scaling & Translation**\n",
        "\n",
        "Since these are **linear operations**, we can **combine them with the next linear layer** (like the attention or MLP weight).\n",
        "‚Üí So instead of:\n",
        "\n",
        "```text\n",
        "LayerNorm ‚Üí Linear Layer\n",
        "```\n",
        "\n",
        "We do:\n",
        "\n",
        "```text\n",
        "One folded Linear Layer\n",
        "```\n",
        "\n",
        "That already includes the LayerNorm's scale and bias.\n",
        "\n",
        "#### Folding Step 3: **Normalization**\n",
        "\n",
        "This part **cannot be folded** (since it's nonlinear).\n",
        "TransformerLens still gives you access via `hook_scale`, so you can track or control it if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Effect on Final Output\n",
        "\n",
        "After folding LayerNorm, we **simplify** the model while keeping the same overall computation.\n",
        "But there's a side effect:\n",
        "A **bias vector** is now added at the final unembedding layer (like a shortcut leftover from the final LayerNorm).\n",
        "\n",
        "üëâ This bias turns out to encode **unigram frequency** ‚Äî boosting common tokens and lowering rare ones in the output logits!\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Visual Analogy\n",
        "\n",
        "| Step                 | No Folding                   | With Folding                            |\n",
        "| -------------------- | ---------------------------- | --------------------------------------- |\n",
        "| Residual ‚Üí LayerNorm | Apply centering + scaling    | Weights already centered (no LayerNorm) |\n",
        "| LayerNorm ‚Üí Linear   | Apply LayerNorm, then Linear | Single folded Linear Layer              |\n",
        "| Final logits         | From unembed + LayerNorm     | From unembed + folded bias              |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pop-llSCkTEl"
      },
      "source": [
        "Great ‚Äî let‚Äôs break down what your code is doing, step by step:\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Purpose:\n",
        "\n",
        "You're inspecting the **unembedding bias vector** (`b_U`) from the final output layer of the transformer model. This bias comes into play **after LayerNorm folding**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What‚Äôs `model.unembed.b_U`?\n",
        "\n",
        "* It‚Äôs a tensor of shape **`[d_vocab]`**, in this case `[50257]` ‚Äî one value per token in the vocabulary.\n",
        "* It gets **added to the output logits** before softmax, subtly shifting the model‚Äôs preferences (e.g., boosting common tokens).\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Code Explanation:\n",
        "\n",
        "```python\n",
        "unembed_bias = model.unembed.b_U\n",
        "```\n",
        "\n",
        "* Grabs the **bias vector** added at the unembedding step.\n",
        "\n",
        "```python\n",
        "bias_values, bias_indices = unembed_bias.sort(descending=True)\n",
        "```\n",
        "\n",
        "* Sorts the bias values **from highest to lowest**, returning:\n",
        "\n",
        "  * `bias_values`: The sorted bias magnitudes\n",
        "  * `bias_indices`: The indices (token IDs) corresponding to those biases\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ì What Can You Do Next?\n",
        "\n",
        "You can now decode these token IDs to text to **see which tokens are favored or disfavored** by the model:\n",
        "\n",
        "```python\n",
        "top_tokens = model.to_str_tokens(bias_indices[:20])\n",
        "bottom_tokens = model.to_str_tokens(bias_indices[-20:])\n",
        "```\n",
        "\n",
        "This would show you:\n",
        "\n",
        "* üîº **Top tokens**: Most *boosted* in the logits ‚Äî often common words like `\" the\"` or `\"and\"`\n",
        "* üîΩ **Bottom tokens**: Most *penalized* ‚Äî often rare or strange tokens\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8DkrvnxZTYC"
      },
      "outputs": [],
      "source": [
        "unembed_bias = model.unembed.b_U\n",
        "bias_values, bias_indices = unembed_bias.sort(descending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU9dBFb-ZTYC"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "print(f\"Top {top_k} values\")\n",
        "for i in range(top_k):\n",
        "    print(f\"{bias_values[i].item():.2f} {repr(model.to_string(bias_indices[i]))}\")\n",
        "\n",
        "print(\"...\")\n",
        "print(f\"Bottom {top_k} values\")\n",
        "for i in range(top_k, 0, -1):\n",
        "    print(f\"{bias_values[-i].item():.2f} {repr(model.to_string(bias_indices[-i]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6nttj4dj_U9"
      },
      "source": [
        "üí° What is this?\n",
        "\n",
        "These are the tokens that the model tends to favor when producing output ‚Äî even before it considers the actual input!\n",
        "\n",
        "Each number (like 7.03) is the bias value for that token in the output layer. A higher bias means the model is more likely to predict that token in general, regardless of context.\n",
        "üß† Why do these tokens rank high?\n",
        "\n",
        "Look at the tokens:\n",
        "\n",
        "    ' the', ' and', ' a', ' of', ' in', ' to', ' is', 'for', etc.\n",
        "\n",
        "    These are very common words in English.\n",
        "\n",
        "    Punctuation like ',', '.', '(', '\"', '\\n' also appear ‚Äî used frequently across text.\n",
        "\n",
        "This bias vector reflects unigram frequency ‚Äî how common each token is across the model‚Äôs training data.\n",
        "\n",
        "So, this bias helps the model by giving it a head start on likely tokens, which:\n",
        "\n",
        "    Makes learning easier and more stable\n",
        "\n",
        "    Helps guide early predictions\n",
        "\n",
        "    Encodes prior knowledge of language frequency\n",
        "\n",
        "üìå Key Takeaway\n",
        "\n",
        "Even without input, the model ‚Äúknows‚Äù that commas and the word \"the\" are common ‚Äî and this gets encoded into the unembedding bias after LayerNorm is folded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hREBORjfeiih"
      },
      "source": [
        "Yes ‚Äî you're almost exactly right. Let me restate and refine your understanding a bit for clarity:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ You're correct that:\n",
        "\n",
        "1. **After folding LayerNorm**, the model ends up with a bias term (`unembed_bias`) in the **final unembedding layer**.\n",
        "\n",
        "2. This `unembed_bias` reflects **token frequency patterns** from the training data:\n",
        "\n",
        "   * High bias for very **common tokens** (e.g. `\" the\"`, `\",\"`, `\"and\"`).\n",
        "   * Low bias for **rare or strange tokens**.\n",
        "\n",
        "3. This bias shifts the **logits** (scores before softmax), making common words more likely **regardless of input context**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç And you're also right that:\n",
        "\n",
        "> ‚ÄúAttention nodes focus on valuable words depending on attention score ‚Äî not the bias.‚Äù\n",
        "\n",
        "Exactly:\n",
        "\n",
        "* **Attention heads** compute **context-dependent interactions** between tokens ‚Äî deciding *what matters* in this specific sentence.\n",
        "* They **ignore the global frequency** of words. Attention is driven by:\n",
        "\n",
        "  * Query‚ÄìKey dot products (i.e., similarity of meaning or structure).\n",
        "  * Not whether a word is common or rare overall.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary (in plain terms):\n",
        "\n",
        "> The **unembedding bias** is like the model‚Äôs *default guess* based on how often words show up in training.\n",
        ">\n",
        "> The **attention layers**, in contrast, look at *what matters* right now, depending on the sentence.\n",
        "\n",
        "Both play a role:\n",
        "\n",
        "* üß± The bias helps with general fluency and structure.\n",
        "* üîç Attention helps with meaning and relationships between words.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQATIlMrehs9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGciAkLmZTYD"
      },
      "source": [
        "This can have real consequences for interpretability - for example, this bias favours \" John\" over \" Mary\" by about 1.2, about 1/3 of the effect size of the Indirect Object Identification Circuit! All other things being the same, this makes the John token 3.6x times more likely than the Mary token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pExEE0xZTYE"
      },
      "outputs": [],
      "source": [
        "john_bias = model.unembed.b_U[model.to_single_token(' John')]\n",
        "mary_bias = model.unembed.b_U[model.to_single_token(' Mary')]\n",
        "\n",
        "print(f\"John bias: {john_bias.item():.4f}\")\n",
        "print(f\"Mary bias: {mary_bias.item():.4f}\")\n",
        "print(f\"Prob ratio bias: {torch.exp(john_bias - mary_bias).item():.4f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plNQ48amlYLp"
      },
      "source": [
        "üìä What the numbers mean:\n",
        "\n",
        "    John bias: 2.8995\n",
        "    ‚Üí The unembed layer gives \"John\" a logit bias of about 2.9.\n",
        "\n",
        "    Mary bias: 1.6034\n",
        "    ‚Üí \"Mary\" is also favored, but less strongly.\n",
        "\n",
        "    Prob ratio bias: 3.6550x\n",
        "    ‚Üí This means:\n",
        "    prob(John)prob(Mary)‚âàe2.8995‚àí1.6034=e1.2961‚âà3.66prob(Mary)prob(John)‚Äã‚âàe2.8995‚àí1.6034=e1.2961‚âà3.66\n",
        "\n",
        "    So without context, the model is ~3.7 times more likely to output \"John\" than \"Mary\", just because \"John\" appeared more often in the training data.\n",
        "\n",
        "üß† Why it matters:\n",
        "\n",
        "This reveals the model's prior bias ‚Äî likely reflecting:\n",
        "\n",
        "    \"John\" being more common in training corpora (like books, web text, etc.).\n",
        "\n",
        "    Gender, cultural, or topic biases embedded in the data.\n",
        "\n",
        "It shows how folded LayerNorm exposes hidden statistical preferences learned during training ‚Äî useful for interpretability and bias analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi0Os5wgZTYE"
      },
      "source": [
        "# Features\n",
        "\n",
        "An overview of some other important features of the library. I recommend checking out the [Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb) for some other important features not mentioned here, and for a demo of what using the library in practice looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OddkdI7ZTYF"
      },
      "source": [
        "## Dealing with tokens\n",
        "\n",
        "**Tokenization** is one of the most annoying features of studying language models. We want language models to be able to take in arbitrary text as input, but the transformer architecture needs the inputs to be elements of a fixed, finite vocabulary. The solution to this is **tokens**, a fixed vocabulary of \"sub-words\", that any natural language can be broken down into with a **tokenizer**. This is invertible, and we can recover the original text, called **de-tokenization**.\n",
        "\n",
        "TransformerLens comes with a range of utility functions to deal with tokenization. Different models can have different tokenizers, so these are all methods on the model.\n",
        "\n",
        "get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token- Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r0rtn0lZTYF"
      },
      "source": [
        "The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n",
        "\n",
        "Some observations - there are a lot of arbitrary-ish details in here!\n",
        "* The tokenizer splits on spaces, so no token contains two words.\n",
        "* Tokens include the preceding space, and whether the first token is a capital letter. `how` and ` how` are different tokens!\n",
        "* Common words are single tokens, even if fairly long (` paragraph`) while uncommon words are split into multiple tokens (` token|ized`).\n",
        "* Tokens *mostly* split on punctuation characters (eg `*` and `.`), but eg `'s` is a single token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBw83pFZZTYF"
      },
      "outputs": [],
      "source": [
        "example_text = \"The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\"\n",
        "example_text_str_tokens = model.to_str_tokens(example_text)\n",
        "print(example_text_str_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8udZ586ZTYG"
      },
      "source": [
        "The transformer needs to take in a sequence of integers, not strings, so we need to convert these tokens into integers. `model.to_tokens` does this, and returns a tensor of integers on the model's device (shape `[batch, position]`). It maps a string to a batch of size 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slVgZGEVZTYH"
      },
      "outputs": [],
      "source": [
        "example_text_tokens = model.to_tokens(example_text)\n",
        "print(example_text_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT45sBBHZTYI"
      },
      "source": [
        "`to_tokens` can also take in a list of strings, and return a batch of size `len(strings)`. If the strings are different numbers of tokens, it adds a PAD token to the end of the shorter strings to make them the same length.\n",
        "\n",
        "(Note: In GPT-2, 50256 signifies both the beginning of sequence, end of sequence and padding token - see the `prepend_bos` section for details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvMCHNa8ZTYJ"
      },
      "outputs": [],
      "source": [
        "example_multi_text = [\"The cat sat on the mat.\", \"The cat sat on the mat really hard.\"]\n",
        "example_multi_text_tokens = model.to_tokens(example_multi_text)\n",
        "print(example_multi_text_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR0sgQx-ZTYK"
      },
      "source": [
        "`model.to_single_token` is a convenience function that takes in a string corresponding to a *single* token and returns the corresponding integer. This is useful for eg looking up the logit corresponding to a single token.\n",
        "\n",
        "For example, let's input `The cat sat on the mat.` to GPT-2, and look at the log prob predicting that the next token is ` The`.\n",
        "\n",
        "<details><summary>Technical notes</summary>\n",
        "\n",
        "Note that if we input a string to the model, it's implicitly converted to a string with `to_tokens`.\n",
        "\n",
        "Note further that the log probs have shape `[batch, position, d_vocab]==[1, 8, 50257]`, with a vector of log probs predicting the next token for *every* token position. GPT-2 uses causal attention which means heads can only look backwards (equivalently, information can only move forwards in the model.), so the log probs at position k are only a function of the first k tokens, and it can't just cheat and look at the k+1 th token. This structure lets it generate text more efficiently, and lets it treat every *token* as a training example, rather than every *sequence*.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVBpo0_tZTYK"
      },
      "outputs": [],
      "source": [
        "cat_text = \"The cat sat on the mat.\"\n",
        "cat_logits = model(cat_text)\n",
        "cat_probs = cat_logits.softmax(dim=-1)\n",
        "print(f\"Probability tensor shape [batch, position, d_vocab] == {cat_probs.shape}\")\n",
        "\n",
        "capital_the_token_index = model.to_single_token(\" The\")\n",
        "print(f\"| The| probability: {cat_probs[0, -1, capital_the_token_index].item():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsl9CTVQZTYM"
      },
      "source": [
        "`model.to_string` is the inverse of `to_tokens` and maps a tensor of integers to a string or list of strings. It also works on integers and lists of integers.\n",
        "\n",
        "For example, let's look up token 256 (due to technical details of tokenization, this will be the most common pair of ASCII characters!), and also verify that our tokens above map back to a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy_KduAvnOM6"
      },
      "source": [
        "Simplified:\n",
        "Certainly! Here‚Äôs a line-by-line explanation of the code:\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "cat_text = \"The cat sat on the mat.\"\n",
        "```\n",
        "- **Assigns a string to the variable `cat_text`.**\n",
        "- This is the text that will be fed into the model.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "cat_logits = model(cat_text)\n",
        "```\n",
        "- **Passes the text to the language model (`model`).**\n",
        "- The output, `cat_logits`, is a tensor containing the raw scores (logits) for each token at each position in the input sequence.\n",
        "- Shape is typically `[batch, sequence_length, vocab_size]`.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "cat_probs = cat_logits.softmax(dim=-1)\n",
        "```\n",
        "- **Applies the softmax function to the logits along the last dimension (`vocab_size`).**\n",
        "- Converts logits into probabilities for each possible token at every position.\n",
        "- `cat_probs` is now a tensor of probabilities (same shape as `cat_logits`).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(f\"Probability tensor shape [batch, position, d_vocab] == {cat_probs.shape}\")\n",
        "```\n",
        "- **Prints the shape of the `cat_probs` tensor.**\n",
        "- `batch`: Number of sequences processed at once (usually 1).\n",
        "- `position`: Number of tokens in the input sequence.\n",
        "- `d_vocab`: Size of the vocabulary (number of possible tokens).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "capital_the_token_index = model.to_single_token(\" The\")\n",
        "```\n",
        "- **Gets the token index for the token `\" The\"` (note the space at the start).**\n",
        "- This is needed because tokenizers often split text based on spaces and subwords.\n",
        "- `capital_the_token_index` is the integer index in the vocabulary corresponding to `\" The\"`.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(f\"| The| probability: {cat_probs[0, -1, capital_the_token_index].item():.2%}\")\n",
        "```\n",
        "- **Extracts and prints the probability that the model assigns to the token `\" The\"` at the last position of the input sequence.**\n",
        "  - `cat_probs[0, -1, capital_the_token_index]`:\n",
        "    - `0`: First (and only) batch.\n",
        "    - `-1`: Last position in the sequence (after the final token).\n",
        "    - `capital_the_token_index`: Probability for the token `\" The\"`.\n",
        "- Converts the probability to a percentage with two decimal places.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "This code:\n",
        "- Feeds a sentence to a language model.\n",
        "- Converts output logits to probabilities.\n",
        "- Finds out how likely the model is to predict the token `\" The\"` as the next token after the sequence.\n",
        "- Prints the probability.\n",
        "\n",
        "---\n",
        "\n",
        "**Let me know if you want a deeper explanation of how tokenization or the softmax works!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU7CpdjpZTYN"
      },
      "outputs": [],
      "source": [
        "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
        "# Squeeze means to remove dimensions of length 1.\n",
        "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
        "# Rank 2 tensors map to a list of strings\n",
        "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3D2YTPMrekJ"
      },
      "source": [
        "1\tDecodes token ID 256 and prints its string value.\n",
        "2-4\tComment: Explains use of .squeeze() to remove singleton batch dimensions.\n",
        "5\tDecodes a tensor of token IDs back to text and prints it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVCikKwkZTYO"
      },
      "source": [
        "A related annoyance of tokenization is that it's hard to figure out how many tokens a string will break into. `model.get_token_position(single_token, tokens)` returns the position of `single_token` in `tokens`. `tokens` can be either a string or a tensor of tokens.\n",
        "\n",
        "Note that position is zero-indexed, it's two (ie third) because there's a beginning of sequence token automatically prepended (see the next section for details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTZ0JCnsZTYP"
      },
      "outputs": [],
      "source": [
        "print(\"With BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\"))\n",
        "print(\"Without BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\", prepend_bos=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7FsfpUZTYQ"
      },
      "source": [
        "If there are multiple copies of the token, we can set `mode=\"first\"` to find the first occurrence's position and `mode=\"last\"` to find the last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FnP7b1TfZTYQ",
        "outputId": "a8cad28d-8a5a-4d04-fcca-81be107cd707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First occurrence 2\n",
            "Final occurrence 13\n"
          ]
        }
      ],
      "source": [
        "print(\"First occurrence\", model.get_token_position(\n",
        "    \" cat\",\n",
        "    \"The cat sat on the mat. The mat sat on the cat.\",\n",
        "    mode=\"first\"))\n",
        "print(\"Final occurrence\", model.get_token_position(\n",
        "    \" cat\",\n",
        "    \"The cat sat on the mat. The mat sat on the cat.\",\n",
        "    mode=\"last\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHIzBEA6ZTYR"
      },
      "source": [
        "In general, tokenization is a pain, and full of gotchas. I highly recommend just playing around with different inputs and their tokenization and getting a feel for it. As another \"fun\" example, let's look at the tokenization of arithmetic expressions - tokens do *not* contain consistent numbers of digits. (This makes it even more impressive that GPT-3 can do arithmetic!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gDhyOPARZTYR",
        "outputId": "08065583-1f10-4134-aba6-fef248898d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', '23', '42', '+', '2017', '=', '214', '45']\n",
            "['<|endoftext|>', '1000', '+', '1', '000000', '=', '9999', '99']\n"
          ]
        }
      ],
      "source": [
        "print(model.to_str_tokens(\"2342+2017=21445\"))\n",
        "print(model.to_str_tokens(\"1000+1000000=999999\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h21u7_jZTYR"
      },
      "source": [
        "I also *highly* recommend investigating prompts with easy tokenization when starting out - ideally key words should form a single token, be in the same position in different prompts, have the same total length, etc. Eg study Indirect Object Identification with common English names like ` Tim` rather than ` Ne|el`. Transformers need to spend some parameters in early layers converting multi-token words to a single feature, and then de-converting this in the late layers, and unless this is what you're explicitly investigating, this will make the behaviour you're investigating be messier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEqvOHH1s5Jh"
      },
      "source": [
        "Summary Table\n",
        "Function\tWhat it does\n",
        "get_token_position\tFinds the index of a token in a sequence\n",
        "to_tokens\tConverts text to token IDs\n",
        "to_string\tConverts token IDs to text\n",
        "to_str_tokens\tConverts text/token IDs to string tokens (subwords/words)\n",
        "prepend_bos\tAdds BOS token to the start of a token sequence\n",
        "to_single_token\tConverts a string to its token ID (single token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbsrlJbZTYR"
      },
      "source": [
        "### Gotcha: `prepend_bos`\n",
        "\n",
        "Key Takeaway: **If you get weird off-by-one errors, check whether there's an unexpected `prepend_bos`!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQBSPDsTZTYR"
      },
      "source": [
        "A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt. TransformerLens does this by default, and it can easily trip up new users. Notably, **this includes `model.forward`** (which is what's implicitly used when you do eg `model(\"Hello World\")`). This is called a **Beginning of Sequence (BOS)** token, and it's a special token used to mark the beginning of the sequence. Confusingly, in GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, `<|endoftext|>` with index `50256`.\n",
        "\n",
        "**Gotcha:** You only want to prepend a BOS token at the *start* of a prompt. If you, eg, want to input a question followed by an answer, and want to tokenize these separately, you do *not* want to prepend_bos on the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5OPlK7e2Pne"
      },
      "source": [
        "Simplified\n",
        "Copilot said: Absolutely! Here‚Äôs a simple text example to\n",
        "\n",
        "Absolutely! Here‚Äôs a simple text example to illustrate the prepend_bos \"gotcha\":\n",
        "Suppose you have this prompt:\n",
        "Code\n",
        "\n",
        "Hello, how are you?\n",
        "\n",
        "When you use a tokenizer with prepend_bos=True, it will add a special BOS token at the beginning:\n",
        "Code\n",
        "\n",
        "<BOS> Hello, how are you?\n",
        "\n",
        "(In GPT-2 and TransformerLens, the BOS token is actually <|endoftext|>, so it becomes:)\n",
        "Code\n",
        "\n",
        "<|endoftext|> Hello, how are you?\n",
        "\n",
        "Why does this matter?\n",
        "\n",
        "If you tokenize two separate parts (say, a question and an answer) and use prepend_bos for both:\n",
        "\n",
        "    Question: <|endoftext|> What is the capital of France?\n",
        "    Answer: <|endoftext|> Paris\n",
        "\n",
        "‚Ä¶but you should only prepend BOS to the whole prompt, not both! The correct way is:\n",
        "Code\n",
        "\n",
        "<|endoftext|> What is the capital of France? Paris\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LPOSv11iZTYT",
        "outputId": "d5f6717b-4003-4646-d9bb-30dbb354b9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape by default (with BOS) torch.Size([1, 3, 50257])\n",
            "Logits shape with BOS torch.Size([1, 3, 50257])\n",
            "Logits shape without BOS - only 2 positions! torch.Size([1, 2, 50257])\n"
          ]
        }
      ],
      "source": [
        "print(\"Logits shape by default (with BOS)\", model(\"Hello World\").shape)\n",
        "print(\"Logits shape with BOS\", model(\"Hello World\", prepend_bos=True).shape)\n",
        "print(\"Logits shape without BOS - only 2 positions!\", model(\"Hello World\", prepend_bos=False).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDM5Cq3L3huH"
      },
      "source": [
        "What does this print?\n",
        "\n",
        "    When you call model(\"Hello World\"), by default it prepends a BOS (Beginning of Sequence) token.\n",
        "    This means the input will be:\n",
        "    [BOS] Hello World (so one more token than just the words in \"Hello World\").\n",
        "\n",
        "Let‚Äôs say \"Hello World\" is tokenized into 2 tokens: [\"Hello\", \"World\"]\n",
        "1. With BOS (default or explicitly True):\n",
        "\n",
        "    Token sequence: [BOS] Hello World ‚Üí 3 tokens\n",
        "    Output shape: (1, 3, vocab_size)\n",
        "    (1=batch, 3=tokens, vocab_size=number of possible tokens)\n",
        "\n",
        "2. Without BOS (prepend_bos=False):\n",
        "\n",
        "    Token sequence: Hello World ‚Üí 2 tokens\n",
        "    Output shape: (1, 2, vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erai8U9TZTYT"
      },
      "source": [
        "1. What is prepend_bos?\n",
        "\n",
        "    prepend_bos is a setting that adds a special Beginning Of Sequence (BOS) token to the start of your input text.\n",
        "    This isn‚Äôt always necessary, but can be helpful for some transformer models.\n",
        "\n",
        "2. Why use it?\n",
        "\n",
        "    Transformers sometimes treat the first token of your input in a special or ‚Äúweird‚Äù way.\n",
        "    In long texts (over 1000 tokens), this doesn‚Äôt matter much. But for short prompts, it can cause problems.\n",
        "    Adding a BOS token gives the model a \"dummy\" first token, so it pays proper attention to your real input.\n",
        "\n",
        "3. Which models need it?\n",
        "\n",
        "    Some models are trained with a BOS token and expect it (like OPT and some interpretability models).\n",
        "    Others, like GPT-2 or GPT-Neo, are not trained this way, but adding a BOS token can still help make their behavior easier to understand.\n",
        "\n",
        "4. How to turn it off?\n",
        "\n",
        "    If you don‚Äôt want to add a BOS token, you can turn it off when you create your model:\n",
        "    Python\n",
        "\n",
        "model = HookedTransformer.from_pretrained('gpt2', default_prepend_bos=False)\n",
        "\n",
        "This will stop the model from automatically adding the BOS token to your inputs.\n",
        "\n",
        "For example, the model can get much worse at Indirect Object Identification without a BOS (and with a name as the first token):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ch7zhbl-rhp"
      },
      "source": [
        "Code simplified\n",
        "Code Explanation\n",
        "Python\n",
        "\n",
        "ioi_logits_without_bos = model(\n",
        "    \"Claire and Mary went to the shops, then Mary gave a bottle of milk to\",\n",
        "    prepend_bos=False\n",
        ")\n",
        "\n",
        "    This runs your transformer model on the prompt without prepending a BOS token.\n",
        "    The output, ioi_logits_without_bos, contains the logits (raw scores) for every possible next token at each position in the input sequence.\n",
        "\n",
        "Python\n",
        "\n",
        "mary_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
        "claire_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
        "\n",
        "    These lines extract the logits for the next token being \" Mary\" and \" Claire\" at the very last position of the prompt.\n",
        "        [0, -1, ...]: First batch (0), last token position (-1), specific token index (for \" Mary\" or \" Claire\").\n",
        "        model.to_single_token(\" Mary\"): Gets the token ID for the string \" Mary\" (with a leading space, as required by many tokenizers).\n",
        "\n",
        "Python\n",
        "\n",
        "print(f\"Logit difference without BOS: {(claire_logit_without_bos - mary_logit_without_bos):.3f}\")\n",
        "\n",
        "    Prints the difference between the logits for \" Claire\" and \" Mary\" at the last position.\n",
        "    This tells you which token the model thinks is more likely as the next word: a positive value means \" Claire\" is more likely, negative means \" Mary\" is.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y5oQSIrPZTYT",
        "outputId": "7d1fb95b-32ad-4166-c85d-8637375f723e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logit difference without BOS: 2.782\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ioi_logits_without_bos = model(\"Claire and Mary went to the shops, then Mary gave a bottle of milk to\", prepend_bos=False)\n",
        "mary_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
        "claire_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
        "print(f\"Logit difference without BOS: {(claire_logit_without_bos - mary_logit_without_bos):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9a98vqd_cT_"
      },
      "source": [
        "Copilot said: Here‚Äôs a clear insight from your results: -\n",
        "\n",
        "Here‚Äôs a clear insight from your results:\n",
        "\n",
        "    Logit difference with BOS: 6.754\n",
        "    Logit difference without BOS: 2.782\n",
        "\n",
        "What does this mean?\n",
        "\n",
        "    The logit difference measures how much more likely the model thinks one token (e.g., \"Claire\") should come next compared to another (e.g., \"Mary\").\n",
        "    A higher logit difference means the model is much more confident in its choice.\n",
        "\n",
        "Insight:\n",
        "\n",
        "Adding a BOS token (prepend_bos=True) makes the model much more confident in its prediction for the next word.\n",
        "\n",
        "    With BOS: The model strongly prefers one token (difference is 6.754).\n",
        "    Without BOS: The model is less confident (difference is only 2.782).\n",
        "\n",
        "Why?\n",
        "\n",
        "    The BOS token helps the model \"settle in\" and pay better attention to your actual prompt, especially for short prompts.\n",
        "    Without BOS, the model may treat the first token oddly, reducing its confidence in the next word prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztDyJ-W7-ofp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q929D5MZTYU"
      },
      "source": [
        "Though, note that this also illustrates another gotcha - when `Claire` is at the start of a sentence (no preceding space), it's actually *two* tokens, not one, which probably confuses the relevant circuit. (Note - in this test we put `prepend_bos=False`, because we want to analyse the tokenization of a specific string, not to give an input to the model!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZSk1Kw7PZTYV",
        "outputId": "cd77e7a6-5727-4951-d14f-070b508a7069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Claire| -> [' Claire']\n",
            "|Claire| -> ['Cl', 'aire']\n"
          ]
        }
      ],
      "source": [
        "print(f\"| Claire| -> {model.to_str_tokens(' Claire', prepend_bos=False)}\")\n",
        "print(f\"|Claire| -> {model.to_str_tokens('Claire', prepend_bos=False)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6a_59NZTYX"
      },
      "source": [
        "## Factored Matrix Class\n",
        "\n",
        "In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix $M = AB$, where M is `[large, large]`, but A is `[large, small]` and B is `[small, large]`. This is a common structure in transformers, and the `FactoredMatrix` class is a convenient way to work with these. It implements efficient algorithms for various operations on these, such as computing the trace, eigenvalues, Frobenius norm, singular value decomposition, and products with other matrices. It can (approximately) act as a drop-in replacement for the original matrix, and supports leading batch dimensions to the factored matrix.\n",
        "\n",
        "<details><summary>Why are low-rank factorized matrices useful for transformer interpretability?</summary>\n",
        "\n",
        "As argued in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), an unexpected fact about transformer attention heads is that rather than being best understood as keys, queries and values (and the requisite weight matrices), they're actually best understood as two low rank factorized matrices.\n",
        "* **Where to move information from:** $W_QK = W_Q W_K^T$, used for determining the attention pattern - what source positions to move information from and what destination positions to move them to.\n",
        "    * Intuitively, residual stream -> query and residual stream -> key are linear maps, *and* `attention_score = query @ key.T` is a linear map, so the whole thing can be factored into one big bilinear form `residual @ W_QK @ residual.T`\n",
        "* **What information to move:** $W_OV = W_V W_O$, used to determine what information to copy from the source position to the destination position (weighted by the attention pattern weight from that destination to that source).\n",
        "    * Intuitively, the residual stream is a `[position, d_model]` tensor (ignoring batch). The attention pattern acts on the *position* dimension (where to move information from and to) and the value and output weights act on the *d_model* dimension - ie *what* information is contained at that source position. So we can factor it all into `attention_pattern @ residual @ W_V @ W_O`, and so only need to care about `W_OV = W_V @ W_O`\n",
        "* Note - the internal head dimension is smaller than the residual stream dimension, so the factorization is low rank. (here, `d_model=768` and `d_head=64`)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTTfYPdm4D2a"
      },
      "source": [
        "Transformer attention heads can be reinterpreted as two low‚Äërank matrices instead of separate query, key, value, output projections. Because the head dimension (d_head) is much smaller than the model dimension (d_model), the combined matrices are low rank and can be stored and manipulated efficiently in factored form.\n",
        "\n",
        "Two key low‚Äërank factorizations:\n",
        "\n",
        "    W_QK = W_Q W_K^T (controls where to move information: the attention pattern over positions).\n",
        "    W_OV = W_V W_O (controls what information/content is moved between positions).\n",
        "\n",
        "Benefits of using the factored form (A B instead of full M):\n",
        "\n",
        "    Saves memory (don‚Äôt build the large square matrix explicitly).\n",
        "    Faster for operations like multiplying by vectors/matrices, trace, norms, SVD, eigen or singular values.\n",
        "    Acts as a near drop‚Äëin replacement for the full matrix, even with batch dimensions.\n",
        "    Matches the true structural constraints imposed by head dimensionality.\n",
        "\n",
        "Core intuition: Attention = (decide which positions to read) using W_QK + (decide what features to copy) using W_OV. Both are naturally low rank because information flows through the smaller head space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GZyNDyZZTYZ"
      },
      "source": [
        "### Basic Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdABWYHZZTYb"
      },
      "source": [
        "We can use the basic class directly - let's make a factored matrix directly and look at the basic operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JWcvxMeIZTYb",
        "outputId": "504b00a9-7744-4803-ae8c-dd51b3c0e855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norms:\n",
            "tensor(5.5796)\n",
            "tensor(5.5796)\n",
            "Right dimension: 5, Left dimension: 5, Hidden dimension: 2\n"
          ]
        }
      ],
      "source": [
        "if IN_GITHUB:\n",
        "    torch.manual_seed(50)\n",
        "A = torch.randn(5, 2)\n",
        "B = torch.randn(2, 5)\n",
        "\n",
        "AB = A @ B\n",
        "AB_factor = FactoredMatrix(A, B)\n",
        "print(\"Norms:\")\n",
        "print(AB.norm())\n",
        "print(AB_factor.norm())\n",
        "\n",
        "print(f\"Right dimension: {AB_factor.rdim}, Left dimension: {AB_factor.ldim}, Hidden dimension: {AB_factor.mdim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc_9sJI0ZTYc"
      },
      "source": [
        "We can also look at the eigenvalues and singular values of the matrix. Note that, because the matrix is rank 2 but 5 by 5, the final 3 eigenvalues and singular values are zero - the factored class omits the zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ERxbrhrXZTYc",
        "outputId": "d7c6543f-1b9a-4876-deb6-9251bb251180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eigenvalues:\n",
            "tensor([-1.1921e-07+0.j,  1.3458e+00+0.j,  3.3862e+00+0.j,  5.9846e-08+0.j,\n",
            "        -4.3360e-08+0.j])\n",
            "tensor([1.3458+0.j, 3.3862+0.j])\n",
            "\n",
            "Singular Values:\n",
            "tensor([4.5323e+00, 3.2542e+00, 3.6035e-07, 9.9029e-08, 2.3852e-08])\n",
            "tensor([4.5323, 3.2542])\n"
          ]
        }
      ],
      "source": [
        "# NBVAL_IGNORE_OUTPUT\n",
        "print(\"Eigenvalues:\")\n",
        "print(torch.linalg.eig(AB).eigenvalues)\n",
        "print(AB_factor.eigenvalues)\n",
        "print()\n",
        "print(\"Singular Values:\")\n",
        "print(torch.linalg.svd(AB).S)\n",
        "print(AB_factor.S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe__nwPIZTYd"
      },
      "source": [
        "We can multiply with other matrices - it automatically chooses the smallest possible dimension to factor along (here it's 2, rather than 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QmzzfkmPZTYd",
        "outputId": "94d60845-a6f0-4990-a902-397c52cba6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfactored: torch.Size([5, 300]) tensor(91.7560)\n",
            "Factored: torch.Size([5, 300]) tensor(91.7560)\n",
            "Right dimension: 300, Left dimension: 5, Hidden dimension: 2\n"
          ]
        }
      ],
      "source": [
        "if IN_GITHUB:\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "C = torch.randn(5, 300)\n",
        "\n",
        "ABC = AB @ C\n",
        "ABC_factor = AB_factor @ C\n",
        "print(\"Unfactored:\", ABC.shape, ABC.norm().round(decimals=3))\n",
        "print(\"Factored:\", ABC_factor.shape, ABC_factor.norm().round(decimals=3))\n",
        "print(f\"Right dimension: {ABC_factor.rdim}, Left dimension: {ABC_factor.ldim}, Hidden dimension: {ABC_factor.mdim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnRF3OAfZTYe"
      },
      "source": [
        "If we want to collapse this back to an unfactored matrix, we can use the AB property to get the product:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K2Ijy5vwZTYe",
        "outputId": "329740f9-4905-4e00-b4a0-36ed14f0ba82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(True)\n"
          ]
        }
      ],
      "source": [
        "AB_unfactored = AB_factor.AB\n",
        "print(torch.isclose(AB_unfactored, AB).all())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl4VNY-_ZTYf"
      },
      "source": [
        "### Medium Example: Eigenvalue Copying Scores\n",
        "\n",
        "(This is a more involved example of how to use the factored matrix class, skip it if you aren't following)\n",
        "\n",
        "For a more involved example, let's look at the eigenvalue copying score from [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) of the OV circuit for various heads. The OV Circuit for a head (the factorised matrix $W_OV = W_V W_O$) is a linear map that determines what information is moved from the source position to the destination position. Because this is low rank, it can be thought of as *reading in* some low rank subspace of the source residual stream and *writing to* some low rank subspace of the destination residual stream (with maybe some processing happening in the middle).\n",
        "\n",
        "A common operation for this will just be to *copy*, ie to have the same reading and writing subspace, and to do minimal processing in the middle. Empirically, this tends to coincide with the OV Circuit having (approximately) positive real eigenvalues. I mostly assert this as an empirical fact, but intuitively, operations that involve mapping eigenvectors to different directions (eg rotations) tend to have complex eigenvalues. And operations that preserve eigenvector direction but negate it tend to have negative real eigenvalues. And \"what happens to the eigenvectors\" is a decent proxy for what happens to an arbitrary vector.\n",
        "\n",
        "We can get a score for \"how positive real the OV circuit eigenvalues are\" with $\\frac{\\sum \\lambda_i}{\\sum |\\lambda_i|}$, where $\\lambda_i$ are the eigenvalues of the OV circuit. This is a bit of a hack, but it seems to work well in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVBdaiA2ZTYf"
      },
      "source": [
        "Let's use FactoredMatrix to compute this for every head in the model! We use the helper `model.OV` to get the concatenated OV circuits for all heads across all layers in the model. This has the shape `[n_layers, n_heads, d_model, d_model]`, where `n_layers` and `n_heads` are batch dimensions and the final two dimensions are factorised as `[n_layers, n_heads, d_model, d_head]` and `[n_layers, n_heads, d_head, d_model]` matrices.\n",
        "\n",
        "We can then get the eigenvalues for this, where there are separate eigenvalues for each element of the batch (a `[n_layers, n_heads, d_head]` tensor of complex numbers), and calculate the copying score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jxT_I_eiZTYg",
        "outputId": "cce54653-d1e9-40e1-bed1-6669338989db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FactoredMatrix: Shape(torch.Size([12, 12, 768, 768])), Hidden Dim(64)\n"
          ]
        }
      ],
      "source": [
        "OV_circuit_all_heads = model.OV\n",
        "print(OV_circuit_all_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPC773g6ZTYg"
      },
      "outputs": [],
      "source": [
        "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues\n",
        "print(OV_circuit_all_heads_eigenvalues.shape)\n",
        "print(OV_circuit_all_heads_eigenvalues.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUPhUbBpZTYi"
      },
      "outputs": [],
      "source": [
        "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Xrig31ZTYi"
      },
      "source": [
        "Head 11 in Layer 11 (L11H11) has a high copying score, and if we plot the eigenvalues they look approximately as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU8WUmOwZTYi"
      },
      "outputs": [],
      "source": [
        "scatter(x=OV_circuit_all_heads_eigenvalues[-1, -1, :].real, y=OV_circuit_all_heads_eigenvalues[-1, -1, :].imag, title=\"Eigenvalues of Head L11H11 of GPT-2 Small\", xaxis=\"Real\", yaxis=\"Imaginary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftIT3Fd-ZTYj"
      },
      "source": [
        "We can even look at the full OV circuit, from the input tokens to output tokens: $W_E W_V W_O W_U$. This is a `[d_vocab, d_vocab]==[50257, 50257]` matrix, so absolutely enormous, even for a single head. But with the FactoredMatrix class, we can compute the full eigenvalue copying score of every head in a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJE3Op1uZTYj"
      },
      "outputs": [],
      "source": [
        "full_OV_circuit = model.embed.W_E @ OV_circuit_all_heads @ model.unembed.W_U\n",
        "print(full_OV_circuit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x61Hc2KpZTYk"
      },
      "outputs": [],
      "source": [
        "full_OV_circuit_eigenvalues = full_OV_circuit.eigenvalues\n",
        "print(full_OV_circuit_eigenvalues.shape)\n",
        "print(full_OV_circuit_eigenvalues.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k62UUFrcZTYk"
      },
      "outputs": [],
      "source": [
        "full_OV_copying_score = full_OV_circuit_eigenvalues.sum(dim=-1).real / full_OV_circuit_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(full_OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jFOKb2tZTYl"
      },
      "source": [
        "Interestingly, these are highly (but not perfectly!) correlated. I'm not sure what to read from this, or what's up with the weird outlier heads!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XiRwUtUZTYl"
      },
      "outputs": [],
      "source": [
        "scatter(x=full_OV_copying_score.flatten(), y=OV_copying_score.flatten(), hover_name=[f\"L{layer}H{head}\" for layer in range(12) for head in range(12)], title=\"OV Copying Score for each head in GPT-2 Small\", xaxis=\"Full OV Copying Score\", yaxis=\"OV Copying Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL9xJD_BZTYm"
      },
      "outputs": [],
      "source": [
        "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
        "# Squeeze means to remove dimensions of length 1.\n",
        "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
        "# Rank 2 tensors map to a list of strings\n",
        "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cvKrUqxZTYm"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R9AvPYSZTYn"
      },
      "source": [
        "TransformerLens also has basic text generation functionality, which can be useful for generally exploring what the model is capable of (thanks to Ansh Radhakrishnan for adding this!). This is pretty rough functionality, and where possible I recommend using more established libraries like HuggingFace for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAn6hl-NZTYn"
      },
      "outputs": [],
      "source": [
        "# NBVAL_IGNORE_OUTPUT\n",
        "model.generate(\"(CNN) President Barack Obama caught in embarrassing new scandal\\n\", max_new_tokens=50, temperature=0.7, prepend_bos=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFUfLCrzZTYq"
      },
      "source": [
        "## Hook Points\n",
        "\n",
        "The key part of TransformerLens that lets us access and edit intermediate activations are the HookPoints around every model activation. Importantly, this technique will work for *any* model architecture, not just transformers, so long as you're able to edit the model code to add in HookPoints! This is essentially a lightweight library bundled with TransformerLens that should let you take an arbitrary model and make it easier to study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIUMS6REZTYq"
      },
      "source": [
        "This is implemented by having a HookPoint layer. Each transformer component has a HookPoint for every activation, which wraps around that activation. The HookPoint acts as an identity function, but has a variety of helper functions that allows us to put PyTorch hooks in to edit and access the relevant activation.\n",
        "\n",
        "There is also a `HookedRootModule` class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well, notably `reset_hooks`, `run_with_cache` and `run_with_hooks`.\n",
        "\n",
        "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass.\n",
        "\n",
        "The syntax for a hook is `function(activation, hook)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6xkXYWZTYr"
      },
      "source": [
        "### Toy Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgBraPwTZTYs"
      },
      "source": [
        "\n",
        "Here's a simple example of defining a small network with HookPoints:\n",
        "\n",
        "We define a basic network with two layers that each take a scalar input $x$, square it, and add a constant:\n",
        "$x_0=x$, $x_1=x_0^2+3$, $x_2=x_1^2-4$.\n",
        "\n",
        "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFhDaqiOZTYt"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
        "\n",
        "\n",
        "class SquareThenAdd(nn.Module):\n",
        "    def __init__(self, offset):\n",
        "        super().__init__()\n",
        "        self.offset = nn.Parameter(torch.tensor(offset))\n",
        "        self.hook_square = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The hook_square doesn't change the value, but lets us access it\n",
        "        square = self.hook_square(x * x)\n",
        "        return self.offset + square\n",
        "\n",
        "\n",
        "class TwoLayerModel(HookedRootModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = SquareThenAdd(3.0)\n",
        "        self.layer2 = SquareThenAdd(-4.0)\n",
        "        self.hook_in = HookPoint()\n",
        "        self.hook_mid = HookPoint()\n",
        "        self.hook_out = HookPoint()\n",
        "\n",
        "        # We need to call the setup function of HookedRootModule to build an\n",
        "        # internal dictionary of modules and hooks, and to give each hook a name\n",
        "        super().setup()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We wrap the input and each layer's output in a hook - they leave the\n",
        "        # value unchanged (unless there's a hook added to explicitly change it),\n",
        "        # but allow us to access it.\n",
        "        x_in = self.hook_in(x)\n",
        "        x_mid = self.hook_mid(self.layer1(x_in))\n",
        "        x_out = self.hook_out(self.layer2(x_mid))\n",
        "        return x_out\n",
        "\n",
        "\n",
        "model = TwoLayerModel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAOc9RNkZTYv"
      },
      "source": [
        "\n",
        "We can add a cache, to save the activation at each hook point\n",
        "\n",
        "(There's a custom `run_with_cache` function on the root module as a convenience, which is a wrapper around model.forward that return model_out, cache_object - we could also manually add hooks with `run_with_hooks` that store activations in a global caching dictionary. This is often useful if we only want to store, eg, subsets or functions of some activations.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLo0GcmBZTYv"
      },
      "outputs": [],
      "source": [
        "\n",
        "out, cache = model.run_with_cache(torch.tensor(5.0))\n",
        "print(\"Model output:\", out.item())\n",
        "for key in cache:\n",
        "    print(f\"Value cached at hook {key}\", cache[key].item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujNflkUUZTYw"
      },
      "source": [
        "\n",
        "We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQBs-cb9ZTYw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def set_to_zero_hook(tensor, hook):\n",
        "    print(hook.name)\n",
        "    return torch.tensor(0.0)\n",
        "\n",
        "\n",
        "print(\n",
        "    \"Output after intervening on layer2.hook_scaled\",\n",
        "    model.run_with_hooks(\n",
        "        torch.tensor(5.0), fwd_hooks=[(\"layer2.hook_square\", set_to_zero_hook)]\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcvlMoqvZTYx"
      },
      "source": [
        "## Loading Pre-Trained Checkpoints\n",
        "\n",
        "There are a lot of interesting questions combining mechanistic interpretability and training dynamics - analysing model capabilities and the underlying circuits that make them possible, and how these change as we train the model.\n",
        "\n",
        "TransformerLens supports these by having several model families with checkpoints throughout training. `HookedTransformer.from_pretrained` can load a checkpoint of a model with the `checkpoint_index` (the label 0 to `num_checkpoints-1`) or `checkpoint_value` (the step or token number, depending on how the checkpoints were labelled)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hilBLqLZTYx"
      },
      "source": [
        "\n",
        "Available models:\n",
        "* All of my interpretability-friendly models have checkpoints available, including:\n",
        "    * The toy models - `attn-only`, `solu`, `gelu` 1L to 4L\n",
        "        * These have ~200 checkpoints, taken on a piecewise linear schedule (more checkpoints near the start of training), up to 22B tokens. Labelled by number of tokens seen.\n",
        "    * The SoLU models trained on 80% Web Text and 20% Python Code (`solu-6l` to `solu-12l`)\n",
        "        * Same checkpoint schedule as the toy models, this time up to 30B tokens\n",
        "    * The SoLU models trained on the pile (`solu-1l-pile` to `solu-12l-pile`)\n",
        "        * These have ~100 checkpoints, taken on a linear schedule, up to 15B tokens. Labelled by number of steps.\n",
        "        * The 12L training crashed around 11B tokens, so is truncated.\n",
        "* The Stanford Centre for Research of Foundation Models trained 5 GPT-2 Small sized and 5 GPT-2 Medium sized models (`stanford-gpt2-small-a` to `e` and `stanford-gpt2-medium-a` to `e`)\n",
        "    * 600 checkpoints, taken on a piecewise linear schedule, labelled by the number of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UkJIYB0ZTYy"
      },
      "source": [
        "The checkpoint structure and labels is somewhat messy and ad-hoc, so I mostly recommend using the `checkpoint_index` syntax (where you can just count from 0 to the number of checkpoints) rather than `checkpoint_value` syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps). The helper function `get_checkpoint_labels` tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type of label was used.\n",
        "\n",
        "Here are graphs of the schedules for several checkpointed models: (note that the first 3 use a log scale, latter 2 use a linear scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2XKokQvZTYy"
      },
      "outputs": [],
      "source": [
        "from transformer_lens.loading_from_pretrained import get_checkpoint_labels\n",
        "for model_name in [\"attn-only-2l\", \"solu-12l\", \"stanford-gpt2-small-a\"]:\n",
        "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
        "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Log scale)\", log_y=True, markers=True)\n",
        "for model_name in [\"solu-1l-pile\", \"solu-6l-pile\"]:\n",
        "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
        "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Linear scale)\", log_y=False, markers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvSWXeCkZTYy"
      },
      "source": [
        "### Example: Induction Head Phase Transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvInCQaEZTYz"
      },
      "source": [
        "One of the more interesting results analysing circuit formation during training is the [induction head phase transition](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html). They find a pretty dramatic shift in models during training - there's a brief period where models go from not having induction heads to having them, which leads to the models suddenly becoming much better at in-context learning (using far back tokens to predict the next token, eg over 500 words back). This is enough of a big deal that it leads to a visible *bump* in the loss curve, where the model's rate of improvement briefly increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7UGlHo1ZTYz"
      },
      "source": [
        "As a brief demonstration of the existence of the phase transition, let's load some checkpoints of a two layer model, and see whether they have induction heads. An easy test, as we used above, is to give the model a repeated sequence of random tokens, and to check how good its loss is on the second half. `evals.induction_loss` is a rough util that runs this test on a model.\n",
        "(Note - this is deliberately a rough, non-rigorous test for the purposes of demonstration, eg `evals.induction_loss` by default just runs it on 4 sequences of 384 tokens repeated twice. These results totally don't do the paper justice - go check it out if you want to see the full results!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eH8lwpQZTYz"
      },
      "source": [
        "In the interests of time and memory, let's look at a handful of checkpoints (chosen to be around the phase change), indices `[10, 25, 35, 60, -1]`. These are roughly 22M, 200M, 500M, 1.6B and 21.8B tokens through training, respectively. (I generally recommend looking things up based on indices, rather than checkpoint value!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNmXOJJ-ZTY0"
      },
      "outputs": [],
      "source": [
        "from transformer_lens import evals\n",
        "# We use the two layer model with SoLU activations, chosen fairly arbitrarily as being both small (so fast to download and keep in memory) and pretty good at the induction task.\n",
        "model_name = \"solu-2l\"\n",
        "# We can load a model from a checkpoint by specifying the checkpoint_index, -1 means the final checkpoint\n",
        "checkpoint_indices = [10, 25, 35, 60, -1]\n",
        "checkpointed_models = []\n",
        "tokens_trained_on = []\n",
        "induction_losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvWU-KeZTY0"
      },
      "source": [
        "We load the models, cache them in a list, and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ebc888e75d714bf1b763e93ba1b3ac39",
            "33fae138baa942318c7a9e2c617c3dd7",
            "85f79d3441a44d2288f091c631bba944",
            "8849bddd4550450482932eec7dbdbea6",
            "2949a4edf4ab4f4db97ff3918584450d",
            "d79dbed80f9b420eacdcc43cf6a32e28",
            "185c3f7d378f4d759b6679dedcefd642",
            "dee64d6b7daa4bb786a161b9405fd210",
            "e59fe2e7170d4fe49159d981fc5ee327"
          ]
        },
        "id": "19LgmP9WZTY0",
        "outputId": "71f76073-49f9-419c-ab9b-59fa7788b8c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebc888e75d714bf1b763e93ba1b3ac39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33fae138baa942318c7a9e2c617c3dd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "./checkpoints/tokens_000022282240.pth:   0%|          | 0.00/227M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85f79d3441a44d2288f091c631bba944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8849bddd4550450482932eec7dbdbea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2949a4edf4ab4f4db97ff3918584450d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-2l into HookedTransformer\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d79dbed80f9b420eacdcc43cf6a32e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "./checkpoints/tokens_000187432960.pth:   0%|          | 0.00/227M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-2l into HookedTransformer\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "185c3f7d378f4d759b6679dedcefd642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "./checkpoints/tokens_000528482304.pth:   0%|          | 0.00/227M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-2l into HookedTransformer\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dee64d6b7daa4bb786a161b9405fd210",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "./checkpoints/tokens_001628438528.pth:   0%|          | 0.00/227M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-2l into HookedTransformer\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e59fe2e7170d4fe49159d981fc5ee327",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "./checkpoints/tokens_021780496384.pth:   0%|          | 0.00/227M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-2l into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "if not IN_GITHUB:\n",
        "    for index in checkpoint_indices:\n",
        "        # Load the model from the relevant checkpoint by index\n",
        "        model_for_this_checkpoint = HookedTransformer.from_pretrained(model_name, checkpoint_index=index, device=device)\n",
        "        checkpointed_models.append(model_for_this_checkpoint)\n",
        "\n",
        "        tokens_seen_for_this_checkpoint = model_for_this_checkpoint.cfg.checkpoint_value\n",
        "        tokens_trained_on.append(tokens_seen_for_this_checkpoint)\n",
        "\n",
        "        induction_loss_for_this_checkpoint = evals.induction_loss(model_for_this_checkpoint, device=device).item()\n",
        "        induction_losses.append(induction_loss_for_this_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMvAn-9cZTY1"
      },
      "source": [
        "We can plot this, and see there's a sharp shift from ~200-500M tokens trained on (note the log scale on the x axis). Interestingly, this is notably earlier than the phase transition in the paper, I'm not sure what's up with that.\n",
        "\n",
        "(To contextualise the numbers, the tokens in the random sequence are uniformly chosen from the first 20,000 tokens (out of ~48,000 total), so random performance is at least $\\ln(20000)\\approx 10$. A naive strategy like \"randomly choose a token that's already appeared in the first half of the sequence (384 elements)\" would get $\\ln(384)\\approx 5.95$, so the model is doing pretty well here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pno9a3SFZTY1",
        "outputId": "2a8a39d9-5323-4ae3-9b74-65ee05748cf6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"649e08c5-bae5-4ce7-871b-28ef5d78788a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"649e08c5-bae5-4ce7-871b-28ef5d78788a\")) {                    Plotly.newPlot(                        \"649e08c5-bae5-4ce7-871b-28ef5d78788a\",                        [{\"hovertemplate\":\"Tokens Trained On=%{x}\\u003cbr\\u003eindex=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"x\":[22282240,187432960,528482304,1628438528,21780496384],\"xaxis\":\"x\",\"y\":[0,1,2,3,4],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens Trained On\"},\"type\":\"log\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Induction Loss over training: solu-2l\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('649e08c5-bae5-4ce7-871b-28ef5d78788a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "line(induction_losses, x=tokens_trained_on, xaxis=\"Tokens Trained On\", yaxis=\"Induction Loss\", title=\"Induction Loss over training: solu-2l\", markers=True, log_x=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}